<!DOCTYPE html>
<html lang="en">
 <head>
  <script>
   !function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)
  </script>
  <title data-rh="true">
   How to apply machine learning and deep learning methods to audio analysis
  </title>
  <meta charset="utf-8" data-rh="true"/>
  <meta content="width=device-width,minimum-scale=1,initial-scale=1" data-rh="true" name="viewport"/>
  <meta content="#000000" data-rh="true" name="theme-color"/>
  <meta content="Medium" data-rh="true" name="twitter:app:name:iphone"/>
  <meta content="828256236" data-rh="true" name="twitter:app:id:iphone"/>
  <meta content="Medium" data-rh="true" property="al:ios:app_name"/>
  <meta content="828256236" data-rh="true" property="al:ios:app_store_id"/>
  <meta content="com.medium.reader" data-rh="true" property="al:android:package"/>
  <meta content="542599432471018" data-rh="true" property="fb:app_id"/>
  <meta content="Medium" data-rh="true" property="og:site_name"/>
  <meta content="article" data-rh="true" property="og:type"/>
  <meta content="2019-11-18T14:56:29.390Z" data-rh="true" property="article:published_time"/>
  <meta content="How to apply machine learning and deep learning methods to audio analysis" data-rh="true" name="title"/>
  <meta content="How to apply machine learning and deep learning methods to audio analysis" data-rh="true" property="og:title"/>
  <meta content="How to apply machine learning and deep learning methods to audio analysis" data-rh="true" property="twitter:title"/>
  <meta content="@cometml" data-rh="true" name="twitter:site"/>
  <meta content="medium://p/e160b069e88" data-rh="true" name="twitter:app:url:iphone"/>
  <meta content="medium://p/e160b069e88" data-rh="true" property="al:android:url"/>
  <meta content="medium://p/e160b069e88" data-rh="true" property="al:ios:url"/>
  <meta content="Medium" data-rh="true" property="al:android:app_name"/>
  <meta content="To view the code, training visualizations, and more information about the python example at the end of this post, visit the Comet project page. While much of the writing and literature on deep…" data-rh="true" name="description"/>
  <meta content="Author: Niko Laskaris, Customer Facing Data Scientist, Comet.ml" data-rh="true" property="og:description"/>
  <meta content="Author: Niko Laskaris, Customer Facing Data Scientist, Comet.ml" data-rh="true" property="twitter:description"/>
  <meta content="https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88" data-rh="true" property="og:url"/>
  <meta content="https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88" data-rh="true" property="al:web:url"/>
  <meta content="https://miro.medium.com/max/1200/0*0y39WDJ_ChDIFKu-" data-rh="true" property="og:image"/>
  <meta content="https://miro.medium.com/max/1200/0*0y39WDJ_ChDIFKu-" data-rh="true" name="twitter:image:src"/>
  <meta content="summary_large_image" data-rh="true" name="twitter:card"/>
  <meta content="https://medium.com/@gidim" data-rh="true" property="article:author"/>
  <meta content="Gideon Mendels" data-rh="true" name="author"/>
  <meta content="index,follow" data-rh="true" name="robots"/>
  <meta content="unsafe-url" data-rh="true" name="referrer"/>
  <meta data-rh="true" name="twitter:label1" value="Reading time"/>
  <meta data-rh="true" name="twitter:data1" value="13 min read"/>
  <meta content="e160b069e88" data-rh="true" name="parsely-post-id"/>
  <link data-rh="true" href="https://plus.google.com/103654360130207659246" rel="publisher"/>
  <link data-rh="true" href="/osd.xml" rel="search" title="Medium" type="application/opensearchdescription+xml"/>
  <link data-rh="true" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png" rel="apple-touch-icon" sizes="152x152"/>
  <link data-rh="true" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png" rel="apple-touch-icon" sizes="120x120"/>
  <link data-rh="true" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png" rel="apple-touch-icon" sizes="76x76"/>
  <link data-rh="true" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png" rel="apple-touch-icon" sizes="60x60"/>
  <link color="#171717" data-rh="true" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" rel="mask-icon"/>
  <link data-rh="true" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" rel="icon"/>
  <link data-rh="true" href="https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css" id="glyph_link" rel="stylesheet" type="text/css"/>
  <link data-rh="true" href="https://medium.com/@gidim" rel="author"/>
  <link data-rh="true" href="https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88" rel="canonical"/>
  <link data-rh="true" href="android-app://com.medium.reader/https/medium.com/p/e160b069e88" rel="alternate"/>
  <script data-rh="true" type="application/ld+json">
   {"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F0*0y39WDJ_ChDIFKu-"],"url":"https:\u002F\u002Fmedium.com\u002Fcomet-ml\u002Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88","dateCreated":"2019-11-18T14:56:29.390Z","datePublished":"2019-11-18T14:56:29.390Z","dateModified":"2019-11-19T19:26:13.051Z","headline":"How to apply machine learning and deep learning methods to audio analysis","name":"How to apply machine learning and deep learning methods to audio analysis","description":"To view the code, training visualizations, and more information about the python example at the end of this post, visit the Comet project page. While much of the writing and literature on deep…","identifier":"e160b069e88","keywords":["Lite:true","Tag:Machine Learning","Tag:Data Science","Tag:Programming","Tag:Artificial Intelligence","Tag:Deep Learning","Publication:comet-ml","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:4"],"author":{"@type":"Person","name":"Gideon Mendels","url":"https:\u002F\u002Fmedium.com\u002F@gidim"},"creator":["Gideon Mendels"],"publisher":{"@type":"Organization","name":"Comet.ml","url":"https:\u002F\u002Fmedium.com\u002Fcomet-ml","logo":{"@type":"ImageObject","width":104,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F208\u002F1*5YHTtkVBRszQgESSqWS1FA.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002Fcomet-ml\u002Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88"}
  </script>
  <script data-rh="true">
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');
  </script>
  <link as="script" href="https://cdn.optimizely.com/js/16180790160.js" rel="preload"/>
  <style data-fela-rehydration="443" data-fela-type="STATIC" type="text/css">
   html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}
  </style>
  <style data-fela-rehydration="443" data-fela-type="KEYFRAME" type="text/css">
   @-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" type="text/css">
   .a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:fixed}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ai{max-width:1192px}.aj{min-width:0}.ak{width:100%}.al{height:65px}.ao{flex:1 0 auto}.ap{flex:0 0 auto}.aq{visibility:hidden}.ar{margin-left:16px}.as{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.at{font-style:normal}.au{line-height:20px}.av{font-size:15.8px}.aw{letter-spacing:0px}.ax{color:rgba(0, 0, 0, 0.54)}.ay{fill:rgba(0, 0, 0, 0.54)}.az{color:rgba(28, 153, 99, 1)}.ba{fill:rgba(2, 184, 117, 1)}.bb{font-size:inherit}.bc{border:inherit}.bd{font-family:inherit}.be{letter-spacing:inherit}.bf{font-weight:inherit}.bg{padding:0}.bh{margin:0}.bi:hover{cursor:pointer}.bj:hover{color:rgba(11, 59, 40, 1)}.bk:hover{fill:rgba(28, 153, 99, 1)}.bl:focus{outline:none}.bm:disabled{cursor:default}.bn:disabled{color:rgba(3, 168, 124, 0.5)}.bo:disabled{fill:rgba(3, 168, 124, 0.5)}.bp{padding:8px 16px}.bq{background:0}.br{border-color:rgba(2, 184, 117, 1)}.bs:hover{border-color:rgba(28, 153, 99, 1)}.bt{border-radius:4px}.bu{border-width:1px}.bv{border-style:solid}.bw{box-sizing:border-box}.bx{display:inline-block}.by{text-decoration:none}.bz{border-top:1px solid rgba(0, 0, 0, 0.1)}.cb{height:54px}.cc{overflow:hidden}.cd{margin-right:40px}.ce{height:36px}.cf{width:62px}.cg{overflow:auto}.ch{flex:0 1 auto}.ci{list-style-type:none}.cj{line-height:40px}.ck{white-space:nowrap}.cl{overflow-x:auto}.cm{align-items:flex-start}.cn{margin-top:20px}.co{padding-top:20px}.cp{height:80px}.cq{height:20px}.cr{margin-right:15px}.cs{margin-left:15px}.ct:first-child{margin-left:0}.cu{min-width:1px}.cv{background-color:rgba(0, 0, 0, 0.34)}.cw{font-weight:300}.cx{font-size:15px}.cy{text-transform:uppercase}.cz{letter-spacing:1px}.da{color:inherit}.db{fill:inherit}.dc:hover{color:rgba(0, 0, 0, 0.9)}.dd:hover{fill:rgba(0, 0, 0, 0.9)}.de:disabled{color:rgba(0, 0, 0, 0.54)}.df:disabled{fill:rgba(0, 0, 0, 0.54)}.dg{margin-bottom:0px}.dh{height:119px}.dk{padding-left:24px}.dl{padding-right:24px}.dm{margin-left:auto}.dn{margin-right:auto}.do{max-width:728px}.dp{flex-direction:column}.dq{position:absolute}.dr{top:calc(100vh + 100px)}.ds{bottom:calc(100vh + 100px)}.dt{width:10px}.du{pointer-events:none}.dv{word-break:break-word}.dw{word-wrap:break-word}.dx:after{display:block}.dy:after{content:""}.dz:after{clear:both}.ea{max-width:680px}.eb{line-height:1.23}.ec{letter-spacing:0}.ed{color:rgba(0, 0, 0, 0.84)}.ee{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.ef{font-size:40px}.el{margin-bottom:-0.27em}.em{line-height:48px}.en{margin-top:32px}.eo{justify-content:space-between}.es{border-radius:50%}.et{height:48px}.eu{width:48px}.ev{margin-left:12px}.ew{margin-bottom:2px}.ey{font-size:16px}.ez{max-height:20px}.fa{text-overflow:ellipsis}.fb{display:-webkit-box}.fc{-webkit-line-clamp:1}.fd{-webkit-box-orient:vertical}.fe:hover{text-decoration:underline}.ff{margin-left:8px}.fg{padding:0px 8px}.fh{border-color:rgba(0, 0, 0, 0.54)}.fi:hover{color:rgba(0, 0, 0, 0.97)}.fj:hover{fill:rgba(0, 0, 0, 0.97)}.fk:hover{border-color:rgba(0, 0, 0, 0.84)}.fl:disabled{fill:rgba(0, 0, 0, 0.76)}.fm:disabled{border-color:rgba(0, 0, 0, 0.2)}.fn:disabled{cursor:inherit}.fo:disabled:hover{color:rgba(0, 0, 0, 0.54)}.fp:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.fq:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.fr{line-height:18px}.fs{align-items:flex-end}.ga{padding-right:8px}.gb{display:none}.gc{margin-right:-6px}.gd{line-height:1.58}.ge{letter-spacing:-0.004em}.gf{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.gq{margin-bottom:-0.46em}.gr{font-style:italic}.gs{background-repeat:repeat-x}.gt{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.gu{background-size:1px 1px}.gv{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.gw{line-height:1.12}.gx{letter-spacing:-0.022em}.gy{font-weight:600}.hj{margin-bottom:-0.28em}.hp{max-width:1358px}.hv{clear:both}.hw{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.hx{cursor:zoom-in}.hy{position:relative}.hz{z-index:auto}.ia{opacity:0}.ib{transition:opacity 100ms 400ms}.ic{height:100%}.id{will-change:transform}.ie{transform:translateZ(0)}.if{margin:auto}.ig{background-color:rgba(0, 0, 0, 0.05)}.ih{padding-bottom:82.62150220913107%}.ii{filter:blur(20px)}.ij{transform:scale(1.1)}.ik{visibility:visible}.il{background:rgba(255, 255, 255, 1)}.im{line-height:1.4}.in{margin-top:10px}.io{text-align:center}.ir{max-width:846px}.is{padding-bottom:42.31678486997636%}.it{max-width:1080px}.iu{padding-bottom:100%}.iv{max-width:300px}.iw{padding-bottom:65%}.ix{font-weight:700}.iy{max-width:468px}.iz{padding-bottom:12.179487179487179%}.ja{max-width:432px}.jb{padding-bottom:66.66666666666667%}.jc{max-width:400px}.jd{padding-bottom:66.75%}.je{max-width:1400px}.jf{padding-bottom:60.214285714285715%}.jg{max-width:450px}.jh{padding-bottom:47.55555555555556%}.ji{max-width:430px}.jj{padding-bottom:26.976744186046513%}.jk{max-width:665px}.jl{padding-bottom:42.10526315789474%}.jm{line-height:1.18}.jx{margin-bottom:-0.31em}.jy{list-style-type:decimal}.jz{margin-left:30px}.ka{padding-left:0px}.kg{max-width:560px}.kh{padding-bottom:75%}.ki{max-width:415px}.kj{padding-bottom:51.80722891566265%}.kk{padding:20px}.kl{background:rgba(0, 0, 0, 0.05)}.km{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.kn{margin-top:-0.09em}.ko{margin-bottom:-0.09em}.kp{white-space:pre-wrap}.kv{padding-bottom:53.42857142857143%}.kw{max-width:738px}.kx{padding-bottom:34.146341463414636%}.ky{max-width:725px}.kz{padding-bottom:34.758620689655174%}.la{max-width:465px}.lb{padding-bottom:103.2258064516129%}.lc{max-width:952px}.ld{padding-bottom:36.76470588235294%}.le{max-width:1160px}.lf{padding-bottom:74.3103448275862%}.lg{padding-bottom:50.42857142857143%}.lh{will-change:opacity}.li{top:calc(65px + 54px + 40px)}.ll{width:131px}.lm{padding-bottom:28px}.ln{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.lo{font-size:18px}.lp{padding-bottom:20px}.lq{padding-top:2px}.lr{max-height:120px}.ls{-webkit-line-clamp:6}.lt{padding:4px 12px}.lu{padding-top:28px}.lv{margin-bottom:19px}.lw{margin-left:-5px}.lx{margin-right:5px}.ly{outline:0}.lz{border:0}.ma{user-select:none}.mb{cursor:pointer}.mc> svg{pointer-events:none}.md:active{border-style:none}.me{-webkit-user-select:none}.mf:focus{fill:rgba(28, 153, 99, 1)}.mg{margin-top:5px}.mh button{text-align:left}.mi{width:188px}.mj{left:50%}.mk{transform:translateX(406px)}.ml{top:calc(65px + 54px + 14px)}.mn{margin-top:40px}.mo{flex-wrap:wrap}.mp{margin-top:25px}.mq{margin-right:8px}.mr{margin-bottom:8px}.ms{border-radius:3px}.mt{padding:5px 10px}.mu{line-height:22px}.mv{margin-top:15px}.mw{margin-right:16px}.mx{border:1px solid rgba(0, 0, 0, 0.1)}.my{height:60px}.mz{width:60px}.nm:active{border-style:solid}.nn{z-index:2}.np{padding-top:32px}.nq{margin-bottom:25px}.nr{margin-bottom:32px}.ns{min-height:80px}.nx{width:80px}.ny{padding-left:102px}.oa{letter-spacing:0.05em}.ob{margin-bottom:6px}.oc{font-size:28px}.od{line-height:36px}.oe{max-width:555px}.of{line-height:24px}.oh{max-width:550px}.oi{padding-top:25px}.oj{color:rgba(0, 0, 0, 0.76)}.ok{opacity:1}.ol{border:1px solid rgba(2, 184, 117, 1)}.om{margin-top:64px}.on{background-color:rgba(0, 0, 0, 0.02)}.oo{padding:60px 0}.op{background-color:rgba(0, 0, 0, 0.9)}.pg{padding-bottom:48px}.ph{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.pi{margin:0 -12px}.pj{margin:0 12px}.pk{flex:1 1 0}.pl{padding-bottom:12px}.pm:hover{color:rgba(255, 255, 255, 0.99)}.pn:hover{fill:rgba(255, 255, 255, 0.99)}.po:disabled{color:rgba(255, 255, 255, 0.7)}.pp:disabled{fill:rgba(255, 255, 255, 0.7)}.pq{color:rgba(255, 255, 255, 0.98)}.pr{fill:rgba(255, 255, 255, 0.98)}.ps{text-align:inherit}.pt{font-size:21.6px}.pu{letter-spacing:-0.32px}.pv{color:rgba(255, 255, 255, 0.7)}.pw{fill:rgba(255, 255, 255, 0.7)}.px{text-decoration:underline}.py{padding-bottom:8px}.pz{padding-top:8px}.qa{width:200px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (min-width: 1080px)" type="text/css">
   .d{display:none}.ah{margin:0 64px}.ek{margin-top:0.78em}.fz{margin-left:30px}.go{font-size:21px}.gp{margin-top:2em}.hh{font-size:34px}.hi{margin-top:1.95em}.ho{margin-top:0.86em}.hu{margin-top:56px}.jv{font-size:26px}.jw{margin-top:1.72em}.kf{margin-top:1.05em}.ku{margin-top:1.91em}.pd{padding-left:64px}.pe{padding-right:64px}.pf{max-width:1320px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 1079.98px)" type="text/css">
   .e{display:none}.fy{margin-left:30px}.ip{margin-left:auto}.iq{text-align:center}.pa{padding-left:64px}.pb{padding-right:64px}.pc{max-width:1080px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 903.98px)" type="text/css">
   .f{display:none}.fx{margin-left:30px}.ox{padding-left:48px}.oy{padding-right:48px}.oz{max-width:904px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 727.98px)" type="text/css">
   .g{display:none}.am{height:56px}.an{display:flex}.ca{display:block}.di{margin-bottom:0px}.dj{height:110px}.eq{margin-top:32px}.er{flex-direction:column-reverse}.fv{margin-bottom:30px}.fw{margin-left:0px}.nt{margin-bottom:24px}.nu{align-items:center}.nv{width:102px}.nw{position:relative}.nz{padding-left:0}.og{margin-top:24px}.oq{padding:32px 0}.ou{padding-left:24px}.ov{padding-right:24px}.ow{max-width:728px}.qb{width:140px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 551.98px)" type="text/css">
   .h{display:none}.ac{margin:0 24px}.eg{margin-top:0.39em}.ep{margin-top:32px}.ex{margin-bottom:0px}.ft{margin-bottom:30px}.fu{margin-left:0px}.gg{font-size:18px}.gh{margin-top:1.56em}.gz{font-size:30px}.ha{margin-top:1.2em}.hk{margin-top:0.67em}.hq{margin-top:40px}.jn{font-size:24px}.jo{margin-top:1.23em}.kb{margin-top:1.34em}.kq{margin-top:1.41em}.or{padding-left:24px}.os{padding-right:24px}.ot{max-width:552px}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)" type="text/css">
   .i{display:none}.ag{margin:0 64px}.ej{margin-top:0.78em}.gm{font-size:21px}.gn{margin-top:2em}.hf{font-size:34px}.hg{margin-top:1.95em}.hn{margin-top:0.86em}.ht{margin-top:56px}.jt{font-size:26px}.ju{margin-top:1.72em}.ke{margin-top:1.05em}.kt{margin-top:1.91em}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)" type="text/css">
   .j{display:none}.af{margin:0 48px}.ei{margin-top:0.78em}.gk{font-size:21px}.gl{margin-top:2em}.hd{font-size:34px}.he{margin-top:1.95em}.hm{margin-top:0.86em}.hs{margin-top:56px}.jr{font-size:26px}.js{margin-top:1.72em}.kd{margin-top:1.05em}.ks{margin-top:1.91em}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)" type="text/css">
   .k{display:none}.ae{margin:0 24px}.eh{margin-top:0.39em}.gi{font-size:18px}.gj{margin-top:1.56em}.hb{font-size:30px}.hc{margin-top:1.2em}.hl{margin-top:0.67em}.hr{margin-top:40px}.jp{font-size:24px}.jq{margin-top:1.23em}.kc{margin-top:1.34em}.kr{margin-top:1.41em}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="print" type="text/css">
   .ab{display:none}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)" type="text/css">
   .y{transition:transform 300ms ease}.z{will-change:transform}.lj{transition:opacity 200ms}.na{transition:border-color 150ms ease}.nb::before{background:
      radial-gradient(circle, rgba(28, 153, 99, 1) 60%, transparent 70%)
    }.nc::before{border-radius:50%}.nd::before{content:""}.ne::before{display:block}.nf::before{z-index:0}.ng::before{left:0}.nh::before{height:100%}.ni::before{position:absolute}.nj::before{top:0}.nk::before{width:100%}.nl:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.no{transition:fill 200ms ease}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 1198px)" type="text/css">
   .lk{display:none}
  </style>
  <style data-fela-rehydration="443" data-fela-type="RULE" media="all and (max-width: 1230px)" type="text/css">
   .mm{display:none}
  </style>
 </head>
 <body>
  <div id="root">
   <div class="a b c">
    <div class="d e f g h i j k">
    </div>
    <script>
     document.domain = document.domain;
    </script>
    <script>
     window.PARSELY = window.PARSELY || {autotrack: false}
    </script>
    <nav class="r s t u v c w x y z ab">
     <div class="branch-journeys-top">
      <div class="r c">
       <div class="n p">
        <div class="ac ae af ag ah ai aj ak">
         <div class="al n o am an">
          <div class="n o ao w">
           <a aria-label="Homepage" href="/?source=post_page-----e160b069e88----------------------" rel="noopener">
            <svg class="q" height="35" viewBox="5 5 35 35" width="35">
             <path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z">
             </path>
            </svg>
           </a>
          </div>
          <div class="r ap w">
           <div class="n o">
            <div class="n g">
             <div class="aq" id="lo-post-page-navbar-sign-in-link">
              <div class="ar r">
               <span class="as b at au av aw r ax ay">
                <a class="az ba bb bc bd be bf bg bh bi bj bk bl bm bn bo" href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2Fcomet-ml%2Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88&amp;source=post_page-----e160b069e88---------------------nav_reg-" rel="noopener">
                 Sign in
                </a>
               </span>
              </div>
             </div>
            </div>
            <div class="aq" id="lo-post-page-navbar-get-started-button">
             <div class="ar r">
              <button class="bp bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl">
               Get started
              </button>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
      <div class="bz r c ca">
       <div class="n p">
        <div class="ac ae af ag ah ai aj ak">
         <div class="cb cc n o">
          <div class="cd r ap">
           <a href="/comet-ml?source=post_page-----e160b069e88----------------------" rel="noopener">
            <div class="ce cf r">
             <img alt="Comet.ml" class="" height="36" src="https://miro.medium.com/max/124/1*5YHTtkVBRszQgESSqWS1FA.png" width="62"/>
            </div>
           </a>
          </div>
          <div class="cg r ch">
           <ul class="ci bh cj ck cl n cm g cn co cp">
            <li class="n o cq cr cs ct">
             <span class="as cw cx au ax cy cz">
              <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="https://medium.com/comet-ml/about?source=post_page-----e160b069e88----------------------" rel="noopener">
               About Comet
              </a>
             </span>
            </li>
            <span class="cq cu cv">
            </span>
            <li class="n o cq cr cs ct">
             <span class="as cw cx au ax cy cz">
              <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="http://comet.ml/?source=post_page-----e160b069e88----------------------" rel="noopener nofollow">
               Comet.ml
              </a>
             </span>
            </li>
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </nav>
    <div class="dg dh r di dj">
    </div>
    <article>
     <section class="dk dl dm dn ak do bw n dp">
     </section>
     <span class="r">
     </span>
     <div>
      <div class="dq u dr ds dt du">
      </div>
      <section class="dv dw dx dy dz">
       <div class="n p">
        <div class="ac ae af ag ah ea aj ak">
         <div>
          <div class="eb ec ed at ee b ef eg eh ei ej ek el" id="f0ec">
           <h1 class="ee b ef em ed">
            How to apply machine learning and deep learning methods to audio analysis
           </h1>
          </div>
          <div class="en">
           <div class="n eo ep eq er">
            <div class="o n">
             <div>
              <a href="/@gidim?source=post_page-----e160b069e88----------------------" rel="noopener">
               <img alt="Gideon Mendels" class="r es et eu" height="48" src="https://miro.medium.com/fit/c/96/96/0*w8Q0QfC3MXUStxPM." width="48"/>
              </a>
             </div>
             <div class="ev ak r">
              <div class="n">
               <div style="flex:1">
                <span class="as b at au av aw r ed q">
                 <div class="ew n o ex">
                  <span class="as cw ey au cc ez fa fb fc fd ed">
                   <a class="da db bb bc bd be bf bg bh bi fe bl bm de df" href="/@gidim?source=post_page-----e160b069e88----------------------" rel="noopener">
                    Gideon Mendels
                   </a>
                  </span>
                  <div class="ff r ap h">
                   <button class="fg ed q bq fh fi fj fk bi de fl fm fn fo fp fq bt as b at fr cx aw bu bv bw bx by bl">
                    Follow
                   </button>
                  </div>
                 </div>
                </span>
               </div>
              </div>
              <span class="as b at au av aw r ax ay">
               <span class="as cw ey au cc ez fa fb fc fd ax">
                <div>
                 <a class="da db bb bc bd be bf bg bh bi fe bl bm de df" href="/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88?source=post_page-----e160b069e88----------------------" rel="noopener">
                  Nov 18
                 </a>
                 <!-- -->
                 ·
                 <!-- -->
                 <!-- -->
                 13
                 <!-- -->
                 min read
                </div>
               </span>
              </span>
             </div>
            </div>
            <div class="n fs ft fu fv fw fx fy fz ab">
             <div class="n o">
              <div class="ga r ap g">
               <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="//medium.com/p/e160b069e88/share/twitter?source=post_actions_header---------------------------" rel="noopener" target="_blank">
                <svg class="q" height="29" width="29">
                 <path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z">
                 </path>
                </svg>
               </a>
              </div>
              <div class="ga r ap g">
               <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="//medium.com/p/e160b069e88/share/facebook?source=post_actions_header---------------------------" rel="noopener" target="_blank">
                <svg class="q" height="29" width="29">
                 <path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79">
                 </path>
                </svg>
               </a>
              </div>
              <div class="ga gb ca">
               <div class="bx">
                <button class="da db bb bc bd be bf bg bh bi dc dd bl bm de df">
                 <svg class="q" height="25" width="25">
                  <g fill-rule="evenodd">
                   <path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01">
                   </path>
                   <path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2">
                   </path>
                  </g>
                 </svg>
                </button>
               </div>
              </div>
              <div class="gc r ao">
               <div>
                <div class="bx">
                 <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fcomet-ml%2Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88&amp;source=post_actions_header--------------------------bookmark_sidebar-" rel="noopener">
                  <svg height="25" viewBox="0 0 25 25" width="25">
                   <path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd">
                   </path>
                  </svg>
                 </a>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="e04d">
          <em class="gr">
           Author: Niko Laskaris, Customer Facing Data Scientist,
          </em>
          <a class="da by gs gt gu gv" href="http://www.comet.ml" rel="noopener nofollow" target="_blank">
           <em class="gr">
            Comet.ml
           </em>
          </a>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="b860">
          To view the code, training visualizations, and more information about the python example at the end of this post, visit the
          <a class="da by gs gt gu gv" href="https://www.comet.ml/demo/urbansound8k/view/" rel="noopener nofollow" target="_blank">
           Comet project page
          </a>
          .
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="f16f">
          <strong class="bf">
           Introduction
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="1236">
          While much of the writing and literature on deep learning concerns computer vision and
          <a class="da by gs gt gu gv" href="https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32" rel="noopener nofollow" target="_blank">
           natural language processing (NLP)
          </a>
          , audio analysis — a field that includes
          <a class="da by gs gt gu gv" href="https://pdfs.semanticscholar.org/5129/350ec0bd8f1fe78a9b864865709f8d8de058.pdf" rel="noopener nofollow" target="_blank">
           automatic speech recognition (ASR)
          </a>
          , digital signal processing, and music classification, tagging, and generation — is a growing subdomain of deep learning applications. Some of the most popular and widespread machine learning systems, virtual assistants Alexa, Siri and Google Home, are largely products built atop models that can extract information from audio signals.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="058a">
          Many of our users at
          <a class="da by gs gt gu gv" href="http://comet.ml" rel="noopener nofollow" target="_blank">
           Comet
          </a>
          are working on audio related machine learning tasks such as audio classification, speech recognition and speech synthesis, so we built them tools to analyze, explore and understand audio data using Comet’s meta machine-learning platform.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn hp">
            <div class="if r hy ig">
             <div class="ih r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="1122" role="presentation" src="https://miro.medium.com/max/60/0*nfA3WmBDB6UIhlKB?q=20" width="1358"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="1122" role="presentation" width="1358"/>
              <noscript>
               <img class="dq t u ic ak" height="1122" role="presentation" src="https://miro.medium.com/max/2716/0*nfA3WmBDB6UIhlKB" width="1358"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Audio modeling, training and debugging using
           <a class="da by gs gt gu gv" href="http://www.comet.ml" rel="noopener nofollow" target="_blank">
            Comet
           </a>
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="79c0">
          This post is focused on showing how data scientists and AI practitioners can use Comet to apply machine learning and deep learning methods in the domain of audio analysis. To understand how models can extract information from digital audio signals, we’ll dive into some of the core feature engineering methods for audio analysis. We will then use
          <a class="da by gs gt gu gv" href="https://librosa.github.io/librosa/" rel="noopener nofollow" target="_blank">
           Librosa
          </a>
          , a great python library for audio analysis, to code up a short python example training a neural architecture on the
          <a class="da by gs gt gu gv" href="https://urbansounddataset.weebly.com/urbansound8k.html" rel="noopener nofollow" target="_blank">
           UrbanSound8k
          </a>
          dataset.
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="80a9">
          <strong class="bf">
           Machine Learning for Audio: Digital Signal Processing, Filter Banks, Mel-Frequency Cepstral Coefficients
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="6356">
          Building machine learning models to classify, describe, or generate audio typically concerns modeling tasks where the input data are audio samples.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn ir">
            <div class="if r hy ig">
             <div class="is r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="358" role="presentation" src="https://miro.medium.com/max/60/0*iSexNcoB8e7Uc2EQ?q=20" width="846"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="358" role="presentation" width="846"/>
              <noscript>
               <img class="dq t u ic ak" height="358" role="presentation" src="https://miro.medium.com/max/1692/0*iSexNcoB8e7Uc2EQ" width="846"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Example waveform of an audio dataset sample from UrbanSound8k
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="7490">
          These audio samples are usually represented as time series, where the y-axis measurement is the amplitude of the waveform. The amplitude is usually measured as a function of the change in pressure around the microphone or receiver device that originally picked up the audio. Unless there is metadata associated with your audio samples, these time series signals will often be your only input data for fitting a model.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="39f5">
          Looking at the samples below, taken from each of the ten classes in the Urbansound8k dataset, it is clear from an eye test that the waveform itself may not necessarily yield clear class identifying information. Consider the waveforms for the engine_idling, siren, and jackhammer classes — they look quite similar.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn it">
            <div class="if r hy ig">
             <div class="iu r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="1080" role="presentation" src="https://miro.medium.com/max/60/0*wVK_KAiSPgE4BMP3?q=20" width="1080"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="1080" role="presentation" width="1080"/>
              <noscript>
               <img class="dq t u ic ak" height="1080" role="presentation" src="https://miro.medium.com/max/2160/0*wVK_KAiSPgE4BMP3" width="1080"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="43cb">
          It turns out one of the best features to extract from audio waveforms (and digital signals in general) has been around since the 1980’s and is still state-of-the-art: Mel Frequency Cepstral Coefficients (MFCCs), introduced by
          <a class="da by gs gt gu gv" href="https://users.cs.northwestern.edu/~pardo/courses/eecs352/papers/Davis1980-MFCC.pdf" rel="noopener nofollow" target="_blank">
           Davis and Mermelstein in 1980
          </a>
          . Below we will go through a technical discussion of how MFCCs are generated and why they are useful in audio analysis. This section is somewhat technical, so before we dive in, let’s define a few key terms pertaining to digital signal processing and audio analysis. We’ll link to wikipedia and additional resources if you’d like to dig even deeper.
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="a806">
          <strong class="bf">
           Disordered Yet Useful Terminology
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="c9e2">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)" rel="noopener nofollow" target="_blank">
           Sampling and Sampling Frequency
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn iv">
           <div class="if r hy ig">
            <div class="iw r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="195" role="presentation" src="https://miro.medium.com/max/60/0*_Luh6nRM7AvLZWgn?q=20" width="300"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="195" role="presentation" width="300"/>
             <noscript>
              <img class="dq t u ic ak" height="195" role="presentation" src="https://miro.medium.com/max/600/0*_Luh6nRM7AvLZWgn" width="300"/>
             </noscript>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="4efa">
          In signal processing,
          <strong class="gf ix">
           sampling
          </strong>
          is the reduction of a continuous signal into a series of discrete values. The
          <strong class="gf ix">
           sampling frequency
          </strong>
          or
          <strong class="gf ix">
           rate
          </strong>
          is the number of samples taken over some fixed amount of time. A high sampling frequency results in less information loss but higher computational expense, and low sampling frequencies have higher information loss but are fast and cheap to compute.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="6a9a">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Amplitude" rel="noopener nofollow" target="_blank">
           Amplitude
          </a>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="db2d">
          The
          <strong class="gf ix">
           amplitude
          </strong>
          of a sound wave is a measure of its change over a period (usually of time). Another common definition of amplitude is a function of the magnitude of the difference between a variable’s extreme values.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="5d2a">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener nofollow" target="_blank">
           Fourier Transform
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn iy">
           <div class="if r hy ig">
            <div class="iz r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="57" role="presentation" src="https://miro.medium.com/max/60/0*i-LwRVLHCV8JfgTu?q=20" width="468"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="57" role="presentation" width="468"/>
             <noscript>
              <img class="dq t u ic ak" height="57" role="presentation" src="https://miro.medium.com/max/936/0*i-LwRVLHCV8JfgTu" width="468"/>
             </noscript>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="422e">
          The
          <strong class="gf ix">
           Fourier Transform
          </strong>
          decomposes a function of time (signal) into constituent frequencies. In the same way a musical chord can be expressed by the volumes and frequencies of its constituent notes, a Fourier Transform of a function displays the amplitude (amount) of each frequency present in the underlying function (signal).
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn ja">
           <div class="if r hy ig">
            <div class="jb r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="288" role="presentation" src="https://miro.medium.com/max/60/0*e3Wz9oS14NtCHTjF?q=20" width="432"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="288" role="presentation" width="432"/>
             <noscript>
              <img class="dq t u ic ak" height="288" role="presentation" src="https://miro.medium.com/max/864/0*e3Wz9oS14NtCHTjF" width="432"/>
             </noscript>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Top: a digital signal; Bottom: the Fourier Transform of the signal
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="0e88">
          There are variants of the Fourier Transform including the
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" rel="noopener nofollow" target="_blank">
           Short-time fourier transform
          </a>
          , which is implemented in the Librosa library and involves splitting an audio signal into frames and then taking the Fourier Transform of each frame. In audio processing generally, the Fourier is an elegant and useful way to decompose an audio signal into its constituent frequencies.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="874e">
          *Resources: by far the best video I’ve found on the Fourier Transform is from
          <a class="da by gs gt gu gv" href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;t=1s" rel="noopener nofollow" target="_blank">
           3Blue1Brown
          </a>
          *
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="6802">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Periodogram" rel="noopener nofollow" target="_blank">
           Periodogram
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn jc">
           <div class="if r hy ig">
            <div class="jd r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="267" role="presentation" src="https://miro.medium.com/max/60/0*GJyquwcAOaD7pGRu?q=20" width="400"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="267" role="presentation" width="400"/>
             <noscript>
              <img class="dq t u ic ak" height="267" role="presentation" src="https://miro.medium.com/max/800/0*GJyquwcAOaD7pGRu" width="400"/>
             </noscript>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="ef32">
          In signal processing, a
          <strong class="gf ix">
           periodogram
          </strong>
          is an estimate of the spectral density of a signal. The periodogram above shows the power spectrum of two sinusoidal basis functions of ~30Hz and ~50Hz. The output of a Fourier Transform can be thought of as being (not exactly) essentially a periodogram.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="fdbd">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Spectral_density" rel="noopener nofollow" target="_blank">
           Spectral Density
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn je">
            <div class="if r hy ig">
             <div class="jf r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="843" role="presentation" src="https://miro.medium.com/max/60/0*jH4hECDI91m9HjHM?q=20" width="1400"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="843" role="presentation" width="1400"/>
              <noscript>
               <img class="dq t u ic ak" height="843" role="presentation" src="https://miro.medium.com/max/2800/0*jH4hECDI91m9HjHM" width="1400"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="c1c7">
          The
          <strong class="gf ix">
           power spectrum
          </strong>
          of a time series is a way to describe the distribution of power into discrete frequency components composing that signal. The statistical average of a signal, measured by its frequency content, is called its
          <strong class="gf ix">
           spectrum
          </strong>
          . The
          <strong class="gf ix">
           spectral density
          </strong>
          of a digital signal describes the frequency content of the signal.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="90d3">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Mel_scale" rel="noopener nofollow" target="_blank">
           Mel-Scale
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn jg">
           <div class="if r hy ig">
            <div class="jh r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="214" role="presentation" src="https://miro.medium.com/max/60/0*abynK4WsXCMsQDby?q=20" width="450"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="214" role="presentation" width="450"/>
             <noscript>
              <img class="dq t u ic ak" height="214" role="presentation" src="https://miro.medium.com/max/900/0*abynK4WsXCMsQDby" width="450"/>
             </noscript>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="44db">
          The
          <strong class="gf ix">
           mel-scale
          </strong>
          is a scale of pitches judged by listeners to be equal in distance from one another. The reference point between the mel-scale and normal frequency measurement is arbitrarily defined by assigning the perceptual pitch of 1000 mels to 1000 Hz. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments. The name
          <strong class="gf ix">
           mel
          </strong>
          comes from the word melody to indicate the scale is based on pitch comparisons.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="2307">
          The formula to convert f hertz into m mels is:
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn ji">
            <div class="if r hy ig">
             <div class="jj r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="116" role="presentation" src="https://miro.medium.com/max/60/0*DkrDP3PuoSfM07bf?q=20" width="430"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="116" role="presentation" width="430"/>
              <noscript>
               <img class="dq t u ic ak" height="116" role="presentation" src="https://miro.medium.com/max/860/0*DkrDP3PuoSfM07bf" width="430"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="8720">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Cepstrum" rel="noopener nofollow" target="_blank">
           Cepstrum
          </a>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="d86b">
          The
          <strong class="gf ix">
           cepstrum
          </strong>
          is the result of taking the Fourier Transform of the logarithm of the estimated power spectrum of a signal.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="b742">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener nofollow" target="_blank">
           Stectrogram
          </a>
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn jk">
           <div class="if r hy ig">
            <div class="jl r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="280" role="presentation" src="https://miro.medium.com/max/60/0*KFcSrKDh4MCw5wG6?q=20" width="665"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="280" role="presentation" width="665"/>
             <noscript>
              <img class="dq t u ic ak" height="280" role="presentation" src="https://miro.medium.com/max/1330/0*KFcSrKDh4MCw5wG6" width="665"/>
             </noscript>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Mel-frequency spectrogram of an audio sample in the Urbansound8k dataset
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="5e97">
          A
          <strong class="gf ix">
           spectrogram
          </strong>
          is a visual representation of the spectrum of frequencies of a signal as it varies with time. A nice way to think about spectrograms is as a stacked view of periodograms across some time-interval digital signal.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="f239">
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Cochlea" rel="noopener nofollow" target="_blank">
           Cochlea
          </a>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="dfd8">
          The spiral cavity of the inner ear containing the organ of Corti, which produces nerve impulses in response to sound vibrations.
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="c16f">
          <strong class="bf">
           Preprocessing Audio: Digital Signal Processing Techniques
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="19d9">
          Dataset preprocessing, feature extraction and feature engineering are steps we take to extract information from the underlying data, information that in a machine learning context should be useful for predicting the class of a sample or the value of some target variable. In audio analysis this process is largely based on finding components of an audio signal that can help us distinguish it from other signals.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="e70f">
          MFCCs, as mentioned above, remain a state of the art tool for extracting information from audio samples. Despite libraries like Librosa giving us a python one-liner to compute MFCCs for an audio sample, the underlying math is a bit complicated, so we’ll go through it step by step and include some useful links for further learning.
         </p>
         <h2 class="jm gx ed at as gy jn jo jp jq jr js jt ju jv jw jx" id="ee4f">
          Steps for calculating MFCCs for a given audio sample:
         </h2>
         <ol class="">
          <li class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq jy jz ka" id="0e23">
           Slice the signal into short frames (of time)
          </li>
          <li class="gd ge ed at gf b gg kb gi kc gk kd gm ke go kf gq jy jz ka" id="7279">
           Compute the periodogram estimate of the power spectrum for each frame
          </li>
          <li class="gd ge ed at gf b gg kb gi kc gk kd gm ke go kf gq jy jz ka" id="0e5f">
           Apply the mel filterbank to the power spectra and sum the energy in each filter
          </li>
          <li class="gd ge ed at gf b gg kb gi kc gk kd gm ke go kf gq jy jz ka" id="7e67">
           Take the discrete cosine transform (DCT) of the log filterbank energies
          </li>
         </ol>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="60f3">
          Excellent additional reading on MFCC derivation and computation can be found at blog posts
          <a class="da by gs gt gu gv" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener nofollow" target="_blank">
           here
          </a>
          and
          <a class="da by gs gt gu gv" href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="noopener nofollow" target="_blank">
           here
          </a>
          .
         </p>
         <ol class="">
          <li class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq jy jz ka" id="2a4e">
           <strong class="gf ix">
            Slice the signal into short frames
           </strong>
          </li>
         </ol>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="dfa2">
          Slicing the audio signal into short frames is useful in that it allows us to
          <em class="gr">
           sample
          </em>
          our audio into discrete time-steps. We assume that on short enough time scales the audio signal doesn’t change. Typical values for the duration of the short frames are between 20–40ms. It is also conventional to overlap each frame 10–15ms.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="f252">
          <em class="gr">
           *Note that the overlapping frames will make the features we eventually generate highly correlated. This is the basis for why we have to take the discrete cosine transform at the end of all of this.*
          </em>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="65a6">
          <strong class="gf ix">
           2. Compute the power spectrum for each frame
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="4de6">
          Once we have our frames we need to calculate the power spectrum of each frame. The power spectrum of a time series describes the distribution of power into frequency components composing that signal. According to Fourier analysis, any physical signal can be decomposed into a number of discrete frequencies, or a spectrum of frequencies over a continuous range. The statistical average of a certain signal as analyzed in terms of its frequency content is called its spectrum.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn kg">
           <div class="if r hy ig">
            <div class="kh r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="420" role="presentation" src="https://miro.medium.com/max/60/0*McV5MrqouAwVb6nn?q=20" width="560"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="420" role="presentation" width="560"/>
             <noscript>
              <img class="dq t u ic ak" height="420" role="presentation" src="https://miro.medium.com/max/1120/0*McV5MrqouAwVb6nn" width="560"/>
             </noscript>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Source:
           <a class="da by gs gt gu gv" href="https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html" rel="noopener nofollow" target="_blank">
            University of Maryland, Harmonic Analysis and the Fourier Transform
           </a>
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="9fc8">
          We apply the
          <strong class="gf ix">
           Short-time fourier transform
          </strong>
          to each frame to obtain a power spectra for each.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="cdbf">
          <strong class="gf ix">
           3. Apply the mel filterbank to the power spectra and sum the energy in each filter
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="b15f">
          We still have some work to do once we have our power spectra. The human cochlea does not discern between nearby frequencies well, and this effect only becomes more pronounced as frequencies increase. The
          <strong class="gf ix">
           mel-scale
          </strong>
          is a tool that allows us to approximate the human auditory system’s response more closely than linear frequency bands.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn ki">
           <div class="if r hy ig">
            <div class="kj r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="215" role="presentation" src="https://miro.medium.com/max/60/0*ENJC6MU0Tot6ctyh?q=20" width="415"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="215" role="presentation" width="415"/>
             <noscript>
              <img class="dq t u ic ak" height="215" role="presentation" src="https://miro.medium.com/max/830/0*ENJC6MU0Tot6ctyh" width="415"/>
             </noscript>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Source:
           <a class="da by gs gt gu gv" href="https://labrosa.ee.columbia.edu/doc/HTKBook21/node54.html" rel="noopener nofollow" target="_blank">
            Columbia
           </a>
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="3dd2">
          As can be seen in the visualization above, the mel filters get wider as the frequency increases — we care less about variations at higher frequencies. At low frequencies, where differences are more discernible to the human ear and thus more important in our analysis, the filters are narrow.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="4117">
          The magnitudes from our power spectra, which were found by applying the Fourier transform to our input data, are
          <a class="da by gs gt gu gv" href="https://en.wikipedia.org/wiki/Data_binning" rel="noopener nofollow" target="_blank">
           binned
          </a>
          by correlating them with each triangular Mel filter. This binning is usually applied such that each coefficient is multiplied by the corresponding filter gain, so each Mel filter comes to hold a weighted sum representing the spectral magnitude in that channel.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="144d">
          Once we have our filterbank energies, we take the logarithm of each. This is yet another step motivated by the constraints of human hearing: humans don’t perceive changes in volume on a linear scale. To double the perceived volume of an audio wave, the wave’s energy must increase by a factor of 8. If an audiowave is already high volume (high energy), large variations in that wave’s energy may not sound very different.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="7d5d">
          <strong class="gf ix">
           4. Take the discrete cosine transform (DCT) of the log filterbank energies
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="e195">
          Because our filterbank energies are overlapping (see step 1), there is usually a strong correlation between them. Taking the discrete cosine transform can help decorrelate the energies.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="ab3f">
          *****
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="4392">
          Thankfully for us, the creators of
          <a class="da by gs gt gu gv" href="https://librosa.github.io/librosa/" rel="noopener nofollow" target="_blank">
           Librosa
          </a>
          have abstracted out a ton of this math and made it easy to generate MFCCs for your audio data. Let’s go through a simple python example to show how this analysis looks in action.
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="f1af">
          <strong class="bf">
           EXAMPLE PROJECT: Urbansound8k + Librosa
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="b824">
          We’re going to be fitting a simple neural network (keras + tensorflow backend) to the UrbanSound8k dataset. To begin let’s load our dependencies, including numpy, pandas, keras, scikit-learn, and librosa.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="10de">#### Dependencies ####<br/><br/>#### Import Comet for experiment tracking and visual tools<br/>from comet_ml import Experiment<br/>####<br/><br/>import IPython.display as ipd<br/>import numpy as np<br/>import pandas as pd<br/>import librosa<br/>import matplotlib.pyplot as plt<br/>from scipy.io import wavfile as wav<br/><br/>from sklearn import metrics <br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split <br/><br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation<br/>from keras.optimizers import Adam<br/>from keras.utils import to_categorical</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="d9f4">
          To begin, let’s create a Comet experiment as a wrapper for all of our work. We’ll be able to capture any and all artifacts (audio files, visualizations, model, dataset, system information, training metrics, etc.) automatically.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="2c02">experiment = Experiment(api_key="API_KEY",<br/>                        project_name="urbansound8k")</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="f6fe">
          Let’s load in the dataset and grab a sample for each class from the dataset. We can inspect these samples visually and acoustically using Comet.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="60fb"># Load dataset<br/>df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="f178"># Create a list of the class labels<br/>labels = list(df['class'].unique())<br/><br/># Let's grab a single audio file from each class<br/>files = dict()<br/>for i in range(len(labels)):<br/>    tmp = df[df['class'] == labels[i]][:1].reset_index()<br/>    path = 'UrbanSound8K/audio/fold{}/{}'.format(tmp['fold'][0], tmp['slice_file_name'][0])<br/>    files[labels[i]] = path</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="95f0">
          We can look at the waveforms for each sample using librosa’s display.waveplot function.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="2700">fig = plt.figure(figsize=(15,15))# Log graphic of waveforms to Comet<br/>experiment.log_image('class_examples.png')<br/>fig.subplots_adjust(hspace=0.4, wspace=0.4)<br/>for i, label in enumerate(labels):<br/>    fn = files[label]<br/>    fig.add_subplot(5, 2, i+1)<br/>    plt.title(label)<br/>    data, sample_rate = librosa.load(fn)<br/>    librosa.display.waveplot(data, sr= sample_rate)<br/>plt.savefig('class_examples.png')</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="f7d0">
          We’ll save this graphic to our Comet experiment.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="ef61"># Log graphic of waveforms to Comet<br/>experiment.log_image('class_examples.png')</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn it">
            <div class="if r hy ig">
             <div class="iu r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="1080" role="presentation" src="https://miro.medium.com/max/60/0*gRmnay8M3EBAwX_j?q=20" width="1080"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="1080" role="presentation" width="1080"/>
              <noscript>
               <img class="dq t u ic ak" height="1080" role="presentation" src="https://miro.medium.com/max/2160/0*gRmnay8M3EBAwX_j" width="1080"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="be1f">
          Next, we’ll log the audio files themselves.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="e645"># Log audio files to Comet for debugging<br/>for label in labels:<br/>    fn = files[label]<br/>    experiment.log_audio(fn, metadata = {'name': label})</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="04db">
          Once we log the samples to Comet, we can listen to samples, inspect metadata, and much more right from the UI.
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn je">
            <div class="if r hy ig">
             <div class="kv r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="748" role="presentation" src="https://miro.medium.com/max/60/0*0y39WDJ_ChDIFKu-?q=20" width="1400"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="748" role="presentation" width="1400"/>
              <noscript>
               <img class="dq t u ic ak" height="748" role="presentation" src="https://miro.medium.com/max/2800/0*0y39WDJ_ChDIFKu-" width="1400"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="fe2f">
          <strong class="gf ix">
           Preprocessing
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="d9a0">
          Now we can extract features from our data. We’re going to be using librosa, but we’ll also show another utility, scipy.io, for comparison and to observe some implicit preprocessing that’s happening.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="00ee">fn = 'UrbanSound8K/audio/fold1/191431-9-0-66.wav'<br/>librosa_audio, librosa_sample_rate = librosa.load(fn)<br/>scipy_sample_rate, scipy_audio = wav.read(fn)<br/><br/>print("Original sample rate: {}".format(scipy_sample_rate))<br/>print("Librosa sample rate: {}".format(librosa_sample_rate))</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="bb91">
          Original sample rate: 48000
          <br/>
          Librosa sample rate: 22050
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="3b5f">
          Librosa’s load function will convert the sampling rate to 22.05 KHz automatically. It will also normalize the bit depth between -1 and 1.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="f6e1">print('Original audio file min~max range: {} to {}'.format(np.min(scipy_audio), np.max(scipy_audio)))</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="a00e">print('Librosa audio file min~max range: {0:.2f} to {0:.2f}'.format(np.min(librosa_audio), np.max(librosa_audio)))</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="cb82">
          &gt;Original audio file min~max range: -1869 to 1665
          <br/>
          &gt; Librosa audio file min~max range: -0.05 to -0.05
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="9ba2">
          Librosa also converts the audio signal to mono from stereo.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="100d">plt.figure(figsize=(12, 4))<br/>plt.plot(scipy_audio)<br/>plt.savefig('original_audio.png')<br/>experiment.log_image('original_audio.png')</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn kw">
            <div class="if r hy ig">
             <div class="kx r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="252" role="presentation" src="https://miro.medium.com/max/60/0*khv78FDS0CTmlwgO?q=20" width="738"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="252" role="presentation" width="738"/>
              <noscript>
               <img class="dq t u ic ak" height="252" role="presentation" src="https://miro.medium.com/max/1476/0*khv78FDS0CTmlwgO" width="738"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="62fc">
          <em class="gr">
           Original Audio (note that it’s in stereo — two audio sources)
          </em>
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="b3b2"># Librosa: mono track<br/>plt.figure(figsize=(12,4))<br/>plt.plot(librosa_audio)<br/>plt.savefig('librosa_audio.png')<br/>experiment.log_image('librosa_audio.png')</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn ky">
            <div class="if r hy ig">
             <div class="kz r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="252" role="presentation" src="https://miro.medium.com/max/60/0*NTu8BYsPZmHWIwL7?q=20" width="725"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="252" role="presentation" width="725"/>
              <noscript>
               <img class="dq t u ic ak" height="252" role="presentation" src="https://miro.medium.com/max/1450/0*NTu8BYsPZmHWIwL7" width="725"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="287b">
          <em class="gr">
           Librosa audio: converted to mono
          </em>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="479b">
          <strong class="gf ix">
           Extracting MFCCs from audio using Librosa
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="cb14">
          Remember all the math we went through to understand mel-frequency cepstrum coefficients earlier? Using Librosa, here’s how you extract them from audio (using the librosa_audio we defined above)
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="f14d">mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc = 40)</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="d2a3">
          That’s it!
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="7fcf">print(mfccs.shape)</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="f0b6">
          &gt; (40, 173)
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="fce7">
          Librosa calculated 40 MFCCs over a 173 frame audio sample.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="ad4d">plt.figure(figsize=(8,8))<br/>librosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')<br/>plt.savefig('MFCCs.png')<br/>experiment.log_image('MFCCs.png')</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="dm dn la">
           <div class="if r hy ig">
            <div class="lb r">
             <div class="ia ib dq t u ic ak cc id ie">
              <img class="dq t u ic ak ii ij ik" height="480" role="presentation" src="https://miro.medium.com/max/58/0*GvSfqUQV0m6gFote?q=20" width="465"/>
             </div>
             <img class="ia ib dq t u ic ak il" height="480" role="presentation" width="465"/>
             <noscript>
              <img class="dq t u ic ak" height="480" role="presentation" src="https://miro.medium.com/max/930/0*GvSfqUQV0m6gFote" width="465"/>
             </noscript>
            </div>
           </div>
          </div>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="90d6">
          We’ll define a simple function to extract MFCCs for every file in our dataset.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="dadd">def extract_features(file_name):</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="25cc">audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') <br/>    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)<br/>    mfccs_processed = np.mean(mfccs.T,axis=0)<br/>     <br/>    return mfccs_processed</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="6ff6">
          Now let’s extract features.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="4a3f">features = []<br/><br/># Iterate through each sound file and extract the features <br/>for index, row in metadata.iterrows():</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="86fa">file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row["fold"])+'/',str(row["slice_file_name"]))<br/>    <br/>    class_label = row["class"]<br/>    data = extract_features(file_name)<br/>    <br/>    features.append([data, class_label])<br/><br/># Convert into a Panda dataframe <br/>featuresdf = pd.DataFrame(features, columns=['feature','class_label'])</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="e261">
          We now have a dataframe where each row has a label (class) and a single feature column, comprised of 40 MFCCs.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="c495">featuresdf.head()</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn lc">
            <div class="if r hy ig">
             <div class="ld r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="350" role="presentation" src="https://miro.medium.com/max/60/0*2tt2pt60eGJ_LpzT?q=20" width="952"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="350" role="presentation" width="952"/>
              <noscript>
               <img class="dq t u ic ak" height="350" role="presentation" src="https://miro.medium.com/max/1904/0*2tt2pt60eGJ_LpzT" width="952"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="13a0">featuresdf.iloc[0]['feature']</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="145e">
          array([-2.1579300e+02, 7.1666122e+01, -1.3181377e+02, -5.2091331e+01,-2.2115969e+01, -2.1764181e+01, -1.1183747e+01, 1.8912683e+01,6.7266388e+00, 1.4556893e+01, -1.1782045e+01, 2.3010368e+00, -1.7251305e+01, 1.0052421e+01, -6.0095000e+00, -1.3153191e+00, -1.7693510e+01, 1.1171228e+00, -4.3699470e+00, 7.2629538e+00, -1.1815971e+01, -7.4952612e+00, 5.4577131e+00, -2.9442446e+00, -5.8693886e+00, -9.8654032e-02, -3.2121708e+00, 4.6092505e+00, -5.8293257e+00, -5.3475075e+00, 1.3341187e+00, 7.1307826e+00, -7.9450034e-02, 1.7109241e+00, -5.6942000e+00, -2.9041715e+00, 3.0366952e+00, -1.6827590e+00, -8.8585770e-01, 3.5438776e-01], dtype=float32)
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="bc01">
          Now that we have successfully extracted our features from the underlying audio data, we can build and train a model.
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="9e24">
          <strong class="gf ix">
           Model building and training
          </strong>
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="c9ed">
          We’ll start by converting our MFCCs to numpy arrays, and encoding our classification labels.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="6da7">from sklearn.preprocessing import LabelEncoder<br/>from keras.utils import to_categorical<br/><br/># Convert features and corresponding classification labels into numpy arrays<br/>X = np.array(featuresdf.feature.tolist())<br/>y = np.array(featuresdf.class_label.tolist())<br/><br/># Encode the classification labels<br/>le = LabelEncoder()<br/>yy = to_categorical(le.fit_transform(y))</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="00bc">
          Our dataset will be split into training and test sets.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="4a1e"># split the dataset <br/>from sklearn.model_selection import train_test_split <br/><br/>x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 127)</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="1460">
          Let’s define and compile a simple feedforward neural network architecture.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="7f48">num_labels = yy.shape[1]<br/>filter_size = 2</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="71f8">def build_model_graph(input_shape=(40,)):<br/>    model = Sequential()<br/>    model.add(Dense(256))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.5))<br/>    model.add(Dense(256))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.5))<br/>    model.add(Dense(num_labels))<br/>    model.add(Activation('softmax'))<br/>    # Compile the model<br/>    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')<br/><br/>    return model</span><span class="jm gx ed at km b ey kq kr ks kt ku ko r kp" id="f8ff">model = build_model_graph()</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="fce2">
          Let’s look at a model summary and compute pre-training accuracy.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="5224"># Display model architecture summary <br/>model.summary()<br/><br/># Calculate pre-training accuracy <br/>score = model.evaluate(x_test, y_test, verbose=0)<br/>accuracy = 100*score[1]</span></pre>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn le">
            <div class="if r hy ig">
             <div class="lf r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="862" role="presentation" src="https://miro.medium.com/max/60/0*uRGk70cCRYTd7mJs?q=20" width="1160"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="862" role="presentation" width="1160"/>
              <noscript>
               <img class="dq t u ic ak" height="862" role="presentation" src="https://miro.medium.com/max/2320/0*uRGk70cCRYTd7mJs" width="1160"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
         </figure>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="fbb5">print("Pre-training accuracy: %.4f%%" % accuracy)</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="e4e3">
          Pre-training accuracy: 12.2496%
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="ae02">
          Now it’s time to train our model.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="c020">from keras.callbacks import ModelCheckpoint <br/>from datetime import datetime <br/><br/>num_epochs = 100<br/>num_batch_size = 32<br/><br/>model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="7847">
          Training completed in time:
         </p>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="1c96">
          Even before training completed, Comet keeps track of the key information about our experiment. We can visualize our accuracy and loss curves in real time from the Comet UI (note the orange spin wheel indicates that training is in process).
         </p>
         <figure class="hq hr hs ht hu hv dm dn paragraph-image">
          <div class="hw hx hy hz ak">
           <div class="dm dn je">
            <div class="if r hy ig">
             <div class="lg r">
              <div class="ia ib dq t u ic ak cc id ie">
               <img class="dq t u ic ak ii ij ik" height="706" role="presentation" src="https://miro.medium.com/max/60/0*eE8BpCleWP1dXCOL?q=20" width="1400"/>
              </div>
              <img class="ia ib dq t u ic ak il" height="706" role="presentation" width="1400"/>
              <noscript>
               <img class="dq t u ic ak" height="706" role="presentation" src="https://miro.medium.com/max/2800/0*eE8BpCleWP1dXCOL" width="1400"/>
              </noscript>
             </div>
            </div>
           </div>
          </div>
          <figcaption class="ax ey im in io do dm dn ip iq as cw">
           Comet’s experiment visualization dashboard
          </figcaption>
         </figure>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="7cbc">
          Once trained we can evaluate our model on the train and test data.
         </p>
         <pre class="hq hr hs ht hu kk kl cl"><span class="jm gx ed at km b ey kn ko r kp" id="d0e2"># Evaluating the model on the training and testing set<br/>score = model.evaluate(x_train, y_train, verbose=0)<br/>print("Training Accuracy: {0:.2%}".format(score[1]))<br/><br/>score = model.evaluate(x_test, y_test, verbose=0)<br/>print("Testing Accuracy: {0:.2%}".format(score[1]))</span></pre>
         <p class="gd ge ed at gf b gg gh gi gj gk gl gm gn go gp gq" id="c8d2">
          Training Accuracy: 93.00%
          <br/>
          Testing Accuracy: 87.35%
         </p>
         <h1 class="gw gx ed at as gy gz ha hb hc hd he hf hg hh hi hj" id="8618">
          <strong class="bf">
           Conclusion
          </strong>
         </h1>
         <p class="gd ge ed at gf b gg hk gi hl gk hm gm hn go ho gq" id="ea9b">
          Our model has trained rather well, but there is likely lots of room for improvement, perhaps using Comet’s
          <a class="da by gs gt gu gv" href="https://www.comet.ml/docs/python-sdk/introduction-optimizer/" rel="noopener nofollow" target="_blank">
           Hyperparameter Optimization
          </a>
          tool. In a small amount of code we’ve been able to extract mathematically complex MFCCs from audio data, build and train a neural network to classify audio based on those MFCCs, and evaluate our model on the test data.
         </p>
         <h2 class="jm gx ed at as gy jn jo jp jq jr js jt ju jv jw jx" id="961e">
          <strong class="bf">
           To get started with Comet,
          </strong>
          <a class="da by gs gt gu gv" href="https://www.comet.ml/pricing?opensignup=true" rel="noopener nofollow" target="_blank">
           <strong class="bf">
            click here.
           </strong>
          </a>
          <strong class="bf">
           Comet is 100% FREE for public projects.
          </strong>
         </h2>
         <h2 class="jm gx ed at as gy jn jo jp jq jr js jt ju jv jw jx" id="1099">
          If you are interested in our Enterprise product, which can be installed on-premise,
          <a class="da by gs gt gu gv" href="https://calendly.com/niko_comet" rel="noopener nofollow" target="_blank">
           schedule a demo here
          </a>
          .
         </h2>
        </div>
       </div>
      </section>
     </div>
    </article>
    <div class="ia du lh s mi mj mk ml lj mm">
    </div>
    <div>
     <div class="mn hv n dp p">
      <div class="n p">
       <div class="ac ae af ag ah ea aj ak">
        <div class="n mo">
        </div>
        <div class="n o mo">
        </div>
        <div class="mp r">
         <ul class="bg bh">
          <li class="bx ci mq mr">
           <a class="ms mt by ax r kl mu a b cx" href="/tag/machine-learning">
            Machine Learning
           </a>
          </li>
          <li class="bx ci mq mr">
           <a class="ms mt by ax r kl mu a b cx" href="/tag/data-science">
            Data Science
           </a>
          </li>
          <li class="bx ci mq mr">
           <a class="ms mt by ax r kl mu a b cx" href="/tag/programming">
            Programming
           </a>
          </li>
          <li class="bx ci mq mr">
           <a class="ms mt by ax r kl mu a b cx" href="/tag/artificial-intelligence">
            Artificial Intelligence
           </a>
          </li>
          <li class="bx ci mq mr">
           <a class="ms mt by ax r kl mu a b cx" href="/tag/deep-learning">
            Deep Learning
           </a>
          </li>
         </ul>
        </div>
        <div class="mv n eo ab">
         <div class="n o">
          <div class="mw r hy">
           <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fcomet-ml%2Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88&amp;source=post_actions_footer-----e160b069e88---------------------clap_footer-" rel="noopener">
            <div class="c mx es n o my hy mz na nb nc nd ne nf ng nh ni nj nk nl bs">
             <div class="bg ly lz ma mb mc nm me o il es n p nn u ic dq t ak ba mf bk no">
              <svg height="33" viewBox="0 0 33 33" width="33">
               <path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd">
               </path>
              </svg>
             </div>
            </div>
           </a>
          </div>
          <div class="mg r">
           <div class="mh">
            <h4 class="as cw ey au ed">
             <button class="da db bb bc bd be bf bg bh bi dc dd bl bm de df">
              596 claps
             </button>
            </h4>
           </div>
          </div>
         </div>
         <div class="n o">
          <div class="ga r ap g">
           <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="//medium.com/p/e160b069e88/share/twitter?source=post_actions_footer---------------------------" rel="noopener" target="_blank">
            <svg class="q" height="29" width="29">
             <path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z">
             </path>
            </svg>
           </a>
          </div>
          <div class="ga r ap g">
           <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="//medium.com/p/e160b069e88/share/facebook?source=post_actions_footer---------------------------" rel="noopener" target="_blank">
            <svg class="q" height="29" width="29">
             <path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79">
             </path>
            </svg>
           </a>
          </div>
          <div class="ga gb ca">
           <div class="bx">
            <button class="da db bb bc bd be bf bg bh bi dc dd bl bm de df">
             <svg class="q" height="25" width="25">
              <g fill-rule="evenodd">
               <path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01">
               </path>
               <path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2">
               </path>
              </g>
             </svg>
            </button>
           </div>
          </div>
          <div class="ga r ap">
           <div>
            <div class="bx">
             <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fcomet-ml%2Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88&amp;source=post_actions_footer--------------------------bookmark_sidebar-" rel="noopener">
              <svg height="25" viewBox="0 0 25 25" width="25">
               <path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd">
               </path>
              </svg>
             </a>
            </div>
           </div>
          </div>
          <div class="bx">
           <div class="bx">
            <div class="r ap">
             <button class="da db bb bc bd be bf bg bh bi dc dd bl bm de df">
              <svg class="q" height="25" viewBox="-480.5 272.5 21 21" width="25">
               <path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z">
               </path>
              </svg>
             </button>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="np bz nq mp r ab">
         <div class="nr ns r hy">
          <span class="r nt an nu">
           <div class="r dq nv nw">
            <a href="/@gidim?source=follow_footer--------------------------follow_footer-" rel="noopener">
             <img alt="Gideon Mendels" class="r es cp nx" height="80" src="https://miro.medium.com/fit/c/160/160/0*w8Q0QfC3MXUStxPM." width="80"/>
            </a>
           </div>
           <span class="r">
            <div class="ny r nz">
             <p class="as cw cx au ax cy oa">
              Written by
             </p>
            </div>
            <div class="ny ob n nz">
             <div class="ak n o eo">
              <h2 class="as gy oc od ed">
               <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="/@gidim?source=follow_footer--------------------------follow_footer-" rel="noopener">
                Gideon Mendels
               </a>
              </h2>
              <div class="r g">
               <button class="lt bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl">
                Follow
               </button>
              </div>
             </div>
            </div>
           </span>
          </span>
          <div class="ny oe r nz ca">
           <div class="jg r">
            <h4 class="as cw lo of ax">
            </h4>
           </div>
           <div class="gb og ca">
            <button class="lt bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl">
             Follow
            </button>
           </div>
          </div>
         </div>
         <div class="np r">
         </div>
         <div class="nr ns r hy">
          <span class="r nt an nu">
           <div class="r dq nv nw">
            <a href="/comet-ml?source=follow_footer--------------------------follow_footer-" rel="noopener">
             <img alt="Comet.ml" class="bt nx cp" height="80" src="https://miro.medium.com/fit/c/160/160/1*3ukGil0uKnhGuqLjT26hmg.png" width="80"/>
            </a>
           </div>
           <span class="r">
            <div class="ny ob n nz">
             <div class="ak n o eo">
              <h2 class="as gy oc od ed">
               <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="/comet-ml?source=follow_footer--------------------------follow_footer-" rel="noopener">
                Comet.ml
               </a>
              </h2>
              <div class="r g">
               <div class="bx">
                <button class="lt bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl">
                 Follow
                </button>
               </div>
              </div>
             </div>
            </div>
           </span>
          </span>
          <div class="ny oh r nz ca">
           <div class="jg r">
            <h4 class="as cw lo of ax">
             Comet is doing for machine learning what GitHub did for software. We allow data science teams to automagically track their datasets, code changes, experimentation history and production models creating efficiency, transparency, and reproducibility.
            </h4>
           </div>
           <div class="gb og ca">
            <div class="bx">
             <button class="lt bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl">
              Follow
             </button>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="oi bz r ab">
         <a class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" href="https://medium.com/p/e160b069e88/responses/show?source=follow_footer--------------------------follow_footer-" rel="noopener">
          <span class="oj ok mb">
           <div class="kk ol bt r io ca">
            <span class="az">
             See responses (1)
            </span>
           </div>
          </span>
         </a>
        </div>
       </div>
      </div>
      <div class="om r on ab">
       <div class="n p">
        <div class="ac ae af ag ah ai aj ak">
        </div>
       </div>
      </div>
     </div>
    </div>
    <div class="oo r op oq">
     <section class="dm dn ak bw r or os ot ou ov ow ox oy oz pa pb pc pd pe pf">
      <div class="pg ph nr n eo g">
       <div class="pi n eo">
        <div class="pj r pk">
         <div class="pl r">
          <a class="da db bb bc bd be bf bg bh bi pm pn bl bm po pp" href="https://medium.com/about?autoplay=1&amp;source=post_page-----e160b069e88----------------------" rel="noopener">
           <h4 class="pq pr ps as gy at of pt pu r">
            Discover
            <!-- -->
            Medium
           </h4>
          </a>
         </div>
         <span class="as b at au av aw r pv pw">
          Welcome to a place where words matter. On
          <!-- -->
          Medium
          <!-- -->
          , smart voices and original ideas take center stage - with no ads in sight.
          <!-- -->
          <a class="da db bb bc bd be bf bg bh bi bl bm po pp px" href="https://medium.com/about?autoplay=1&amp;source=post_page-----e160b069e88----------------------" rel="noopener">
           Watch
          </a>
         </span>
        </div>
        <div class="pj r pk">
         <div class="py r">
          <a class="da db bb bc bd be bf bg bh bi pm pn bl bm po pp" href="https://medium.com/topics?source=post_page-----e160b069e88----------------------" rel="noopener">
           <h4 class="pq pr ps as gy at of pt pu r">
            Make
            <!-- -->
            Medium
            <!-- -->
            yours
           </h4>
          </a>
         </div>
         <span class="as b at au av aw r pv pw">
          Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.
          <!-- -->
          <a class="da db bb bc bd be bf bg bh bi bl bm po pp px" href="https://medium.com/topics?source=post_page-----e160b069e88----------------------" rel="noopener">
           Explore
          </a>
         </span>
        </div>
        <div class="pj r pk">
         <div class="pl r">
          <a class="da db bb bc bd be bf bg bh bi pm pn bl bm po pp" href="https://medium.com/membership?source=post_page-----e160b069e88----------------------" rel="noopener">
           <h4 class="pq pr ps as gy at of pt pu r">
            Become a member
           </h4>
          </a>
         </div>
         <span class="as b at au av aw r pv pw">
          Get unlimited access to the best stories on
          <!-- -->
          Medium
          <!-- -->
          — and support writers while you’re at it. Just $5/month.
          <!-- -->
          <a class="da db bb bc bd be bf bg bh bi bl bm po pp px" href="https://medium.com/membership?source=post_page-----e160b069e88----------------------" rel="noopener">
           Upgrade
          </a>
         </span>
        </div>
       </div>
      </div>
      <div class="n o eo">
       <a class="da db bb bc bd be bf bg bh bi pm pn bl bm po pp" href="/?source=post_page-----e160b069e88----------------------" rel="noopener">
        <svg class="pr" height="22" viewBox="0 0 111.5 22" width="112">
         <path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z">
         </path>
        </svg>
       </a>
       <span class="as b at au av aw r pv pw">
        <div class="pz qa n eo qb an">
         <a class="da db bb bc bd be bf bg bh bi fe bl bm po pp" href="https://medium.com/about?autoplay=1&amp;source=post_page-----e160b069e88----------------------" rel="noopener">
          About
         </a>
         <a class="da db bb bc bd be bf bg bh bi fe bl bm po pp" href="https://help.medium.com/?source=post_page-----e160b069e88----------------------" rel="noopener">
          Help
         </a>
         <a class="da db bb bc bd be bf bg bh bi fe bl bm po pp" href="/policy/9db0094a1e0f?source=post_page-----e160b069e88----------------------" rel="noopener">
          Legal
         </a>
        </div>
       </span>
      </div>
     </section>
    </div>
   </div>
  </div>
  <script src="https://cdn.optimizely.com/js/16180790160.js">
  </script>
  <script>
   window.__BUILD_ID__ = "development"
  </script>
  <script>
   window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"
  </script>
  <script>
   window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20191121-172537-39186d1762","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20191121-172537-39186d1762"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"]},"debug":{"requestId":"e328da49-1748-4008-8252-efe4d9d528d5","originalSpanCarrier":{"ot-tracer-spanid":"13b0c8ab28c01ff3","ot-tracer-traceid":"2e8147e7432255f6","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_6yr2ROTOs64M"},"xsrf":""},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hasRenderedBranchBanner":null,"hideGoogleOneTap":false,"hideAlternateUser":false,"branchData":{"loaded":false},"currentLocation":"https:\u002F\u002Fmedium.com\u002Fcomet-ml\u002Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88","host":"medium.com","hostname":"medium.com","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}}},"client":{"isBot":false,"isEu":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}
  </script>
  <script>
   window.__APOLLO_STATE__ = {"ROOT_QUERY":{"viewer":null,"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"}],"meterPost({\"postId\":\"e160b069e88\",\"postMeteringOptions\":{}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"e160b069e88\"})":{"type":"id","generated":false,"id":"Post:e160b069e88","typename":"Post"}},"ROOT_QUERY.variantFlags.0":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.3":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.4":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.5":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.6":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.7":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_continuous_model_deployment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_first_name_on_blocker_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_gift_membership_ended_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_inline_search_lite","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_inline_search_popover_lite","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.30":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_lite_read_next","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagString","value":"sidebar"},"ROOT_QUERY.variantFlags.39":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lo_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lo_scroll_away_metabar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_membership_thank_you_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_more_branch_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_multi_day_pub_digests","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_new_gdpr_partner_export","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_new_google_play_api","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_new_partner_dashboard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_new_partner_program_updates","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_new_paypal_billing_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_popularity_feature_exp_retrain","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_pub_digest_send_after_pub_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.74":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.79":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"remove_for_members_section_on_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.82":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.83":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:e160b069e88":{"__typename":"Post","visibility":"PUBLIC","latestPublishedVersion":"3540af3bc32f","collection":{"type":"id","generated":false,"id":"Collection:aeed698699d2","typename":"Collection"},"id":"e160b069e88","creator":{"type":"id","generated":false,"id":"User:ab67b462fe4b","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","sequence":null,"mediumUrl":"https:\u002F\u002Fmedium.com\u002Fcomet-ml\u002Fapplyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88","canonicalUrl":"","content({\"postMeteringOptions\":{}})":{"type":"id","generated":true,"id":"$Post:e160b069e88.content({\"postMeteringOptions\":{}})","typename":"PostContent"},"firstPublishedAt":1574088989390,"isPublished":true,"layerCake":4,"primaryTopic":null,"title":"How to apply machine learning and deep learning methods to audio analysis","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":12.456603773584906,"readingList":"READING_LIST_NONE","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:data-science","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:programming","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"}],"viewerClapCount":0,"clapCount":596,"voterCount":28,"recommenders":[],"responsesCount":1,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":1574088987522,"audioVersionUrl":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1574088989390,"previewContent":{"type":"id","generated":true,"id":"$Post:e160b069e88.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*0y39WDJ_ChDIFKu-","typename":"ImageMetadata"},"updatedAt":1574191573051,"topics":[],"isSuspended":false},"Collection:aeed698699d2":{"id":"aeed698699d2","domain":null,"slug":"comet-ml","__typename":"Collection","googleAnalyticsId":null,"colorBehavior":"ACCENT_COLOR","name":"Comet.ml","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*5YHTtkVBRszQgESSqWS1FA.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*3ukGil0uKnhGuqLjT26hmg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"creator":{"type":"id","generated":false,"id":"User:ab67b462fe4b","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:aeed698699d2.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:aeed698699d2.navItems.1","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Comet is doing for machine learning what GitHub did for software. We allow data science teams to automagically track their datasets, code changes, experimentation history and production models creating efficiency, transparency, and reproducibility.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"cometml","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:","typename":"ImageMetadata"}},"User:ab67b462fe4b":{"id":"ab67b462fe4b","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Gideon Mendels","isFollowing":false,"username":"gidim","bio":"","imageId":"0*w8Q0QfC3MXUStxPM.","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":""},"ImageMetadata:1*5YHTtkVBRszQgESSqWS1FA.png":{"id":"1*5YHTtkVBRszQgESSqWS1FA.png","originalWidth":876,"originalHeight":504,"__typename":"ImageMetadata"},"ImageMetadata:1*3ukGil0uKnhGuqLjT26hmg.png":{"id":"1*3ukGil0uKnhGuqLjT26hmg.png","__typename":"ImageMetadata"},"Collection:aeed698699d2.navItems.0":{"title":"About Comet","url":"https:\u002F\u002Fmedium.com\u002Fcomet-ml\u002Fabout","type":"ABOUT_PAGE_NAV_ITEM","__typename":"NavItem"},"Collection:aeed698699d2.navItems.1":{"title":"Comet.ml","url":"http:\u002F\u002Fcomet.ml\u002F","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:aeed698699d2.colorPalette":{"tintBackgroundSpectrum":null,"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF02B875","point":0,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF00AB6B","point":0.1,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF1C9963","point":0.2,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF092E20","point":1,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFFFFFFF","point":0,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9FDF0","point":0.1,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE2FAEE","point":0.2,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFADFFCF","point":0.6,"__typename":"ColorPoint"},"$Collection:aeed698699d2.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FF7DFFB3","point":1,"__typename":"ColorPoint"},"$Post:e160b069e88.content({\"postMeteringOptions\":{}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:e160b069e88.content({\"postMeteringOptions\":{}}).bodyModel","typename":"RichText"}},"$Post:e160b069e88.content({\"postMeteringOptions\":{}}).bodyModel.sections.0":{"name":"25e1","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:e160b069e88.content({\"postMeteringOptions\":{}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:e160b069e88.content({\"postMeteringOptions\":{}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_97","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_98","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_99","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_100","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_101","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_102","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_103","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_104","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_105","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_106","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_107","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_108","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_109","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_110","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_111","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_112","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_113","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_114","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_115","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_116","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_117","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_118","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_119","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_120","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_121","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_122","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_123","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_124","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_125","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_126","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_127","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_128","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_129","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_130","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_131","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_132","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_133","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_134","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_135","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_136","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_137","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_138","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_139","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_140","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_141","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_142","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_143","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_144","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_145","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_146","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_147","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_148","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:3540af3bc32f_149","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:3540af3bc32f_0":{"id":"3540af3bc32f_0","name":"f0ec","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"How to apply machine learning and deep learning methods to audio analysis","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_1":{"id":"3540af3bc32f_1","name":"e04d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Author: Niko Laskaris, Customer Facing Data Scientist, Comet.ml","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_1.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_1.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_1.markups.0":{"type":"A","start":55,"end":63,"href":"http:\u002F\u002Fwww.comet.ml","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_1.markups.1":{"type":"EM","start":0,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_2":{"id":"3540af3bc32f_2","name":"b860","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"To view the code, training visualizations, and more information about the python example at the end of this post, visit the Comet project page.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_2.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_2.markups.0":{"type":"A","start":124,"end":142,"href":"https:\u002F\u002Fwww.comet.ml\u002Fdemo\u002Furbansound8k\u002Fview\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_3":{"id":"3540af3bc32f_3","name":"f16f","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Introduction","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_3.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_3.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_4":{"id":"3540af3bc32f_4","name":"1236","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"While much of the writing and literature on deep learning concerns computer vision and natural language processing (NLP), audio analysis — a field that includes automatic speech recognition (ASR), digital signal processing, and music classification, tagging, and generation — is a growing subdomain of deep learning applications. Some of the most popular and widespread machine learning systems, virtual assistants Alexa, Siri and Google Home, are largely products built atop models that can extract information from audio signals.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_4.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_4.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_4.markups.0":{"type":"A","start":87,"end":120,"href":"https:\u002F\u002Fbecominghuman.ai\u002Fa-simple-introduction-to-natural-language-processing-ea66a1747b32","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_4.markups.1":{"type":"A","start":161,"end":195,"href":"https:\u002F\u002Fpdfs.semanticscholar.org\u002F5129\u002F350ec0bd8f1fe78a9b864865709f8d8de058.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_5":{"id":"3540af3bc32f_5","name":"058a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Many of our users at Comet are working on audio related machine learning tasks such as audio classification, speech recognition and speech synthesis, so we built them tools to analyze, explore and understand audio data using Comet’s meta machine-learning platform.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_5.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_5.markups.0":{"type":"A","start":21,"end":26,"href":"http:\u002F\u002Fcomet.ml","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_6":{"id":"3540af3bc32f_6","name":"799b","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*nfA3WmBDB6UIhlKB","typename":"ImageMetadata"},"text":"Audio modeling, training and debugging using Comet","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_6.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*nfA3WmBDB6UIhlKB":{"id":"0*nfA3WmBDB6UIhlKB","originalHeight":1122,"originalWidth":1358,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_6.markups.0":{"type":"A","start":45,"end":50,"href":"http:\u002F\u002Fwww.comet.ml","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_7":{"id":"3540af3bc32f_7","name":"79c0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"This post is focused on showing how data scientists and AI practitioners can use Comet to apply machine learning and deep learning methods in the domain of audio analysis. To understand how models can extract information from digital audio signals, we’ll dive into some of the core feature engineering methods for audio analysis. We will then use Librosa, a great python library for audio analysis, to code up a short python example training a neural architecture on the UrbanSound8k dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_7.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_7.markups.0":{"type":"A","start":347,"end":354,"href":"https:\u002F\u002Flibrosa.github.io\u002Flibrosa\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_7.markups.1":{"type":"A","start":471,"end":483,"href":"https:\u002F\u002Furbansounddataset.weebly.com\u002Furbansound8k.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_8":{"id":"3540af3bc32f_8","name":"80a9","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Machine Learning for Audio: Digital Signal Processing, Filter Banks, Mel-Frequency Cepstral Coefficients","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_8.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_8.markups.0":{"type":"STRONG","start":0,"end":104,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_9":{"id":"3540af3bc32f_9","name":"6356","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Building machine learning models to classify, describe, or generate audio typically concerns modeling tasks where the input data are audio samples.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_10":{"id":"3540af3bc32f_10","name":"5475","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*iSexNcoB8e7Uc2EQ","typename":"ImageMetadata"},"text":"Example waveform of an audio dataset sample from UrbanSound8k","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*iSexNcoB8e7Uc2EQ":{"id":"0*iSexNcoB8e7Uc2EQ","originalHeight":358,"originalWidth":846,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_11":{"id":"3540af3bc32f_11","name":"7490","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"These audio samples are usually represented as time series, where the y-axis measurement is the amplitude of the waveform. The amplitude is usually measured as a function of the change in pressure around the microphone or receiver device that originally picked up the audio. Unless there is metadata associated with your audio samples, these time series signals will often be your only input data for fitting a model.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_12":{"id":"3540af3bc32f_12","name":"39f5","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Looking at the samples below, taken from each of the ten classes in the Urbansound8k dataset, it is clear from an eye test that the waveform itself may not necessarily yield clear class identifying information. Consider the waveforms for the engine_idling, siren, and jackhammer classes — they look quite similar.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_13":{"id":"3540af3bc32f_13","name":"1741","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*wVK_KAiSPgE4BMP3","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*wVK_KAiSPgE4BMP3":{"id":"0*wVK_KAiSPgE4BMP3","originalHeight":1080,"originalWidth":1080,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_14":{"id":"3540af3bc32f_14","name":"43cb","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"It turns out one of the best features to extract from audio waveforms (and digital signals in general) has been around since the 1980’s and is still state-of-the-art: Mel Frequency Cepstral Coefficients (MFCCs), introduced by Davis and Mermelstein in 1980. Below we will go through a technical discussion of how MFCCs are generated and why they are useful in audio analysis. This section is somewhat technical, so before we dive in, let’s define a few key terms pertaining to digital signal processing and audio analysis. We’ll link to wikipedia and additional resources if you’d like to dig even deeper.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_14.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_14.markups.0":{"type":"A","start":226,"end":255,"href":"https:\u002F\u002Fusers.cs.northwestern.edu\u002F~pardo\u002Fcourses\u002Feecs352\u002Fpapers\u002FDavis1980-MFCC.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_15":{"id":"3540af3bc32f_15","name":"a806","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Disordered Yet Useful Terminology","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_15.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_15.markups.0":{"type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_16":{"id":"3540af3bc32f_16","name":"c9e2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Sampling and Sampling Frequency","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_16.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_16.markups.0":{"type":"A","start":0,"end":31,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSampling_(signal_processing)","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_17":{"id":"3540af3bc32f_17","name":"6269","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*_Luh6nRM7AvLZWgn","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*_Luh6nRM7AvLZWgn":{"id":"0*_Luh6nRM7AvLZWgn","originalHeight":195,"originalWidth":300,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_18":{"id":"3540af3bc32f_18","name":"4efa","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In signal processing, sampling is the reduction of a continuous signal into a series of discrete values. The sampling frequency or rate is the number of samples taken over some fixed amount of time. A high sampling frequency results in less information loss but higher computational expense, and low sampling frequencies have higher information loss but are fast and cheap to compute.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_18.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_18.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_18.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_18.markups.0":{"type":"STRONG","start":22,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_18.markups.1":{"type":"STRONG","start":109,"end":127,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_18.markups.2":{"type":"STRONG","start":131,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_19":{"id":"3540af3bc32f_19","name":"6a9a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Amplitude","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_19.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_19.markups.0":{"type":"A","start":0,"end":9,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FAmplitude","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_20":{"id":"3540af3bc32f_20","name":"db2d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The amplitude of a sound wave is a measure of its change over a period (usually of time). Another common definition of amplitude is a function of the magnitude of the difference between a variable’s extreme values.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_20.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_20.markups.0":{"type":"STRONG","start":4,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_21":{"id":"3540af3bc32f_21","name":"5d2a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Fourier Transform","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_21.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_21.markups.0":{"type":"A","start":0,"end":17,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FFourier_transform","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_22":{"id":"3540af3bc32f_22","name":"c53c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*i-LwRVLHCV8JfgTu","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*i-LwRVLHCV8JfgTu":{"id":"0*i-LwRVLHCV8JfgTu","originalHeight":57,"originalWidth":468,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_23":{"id":"3540af3bc32f_23","name":"422e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The Fourier Transform decomposes a function of time (signal) into constituent frequencies. In the same way a musical chord can be expressed by the volumes and frequencies of its constituent notes, a Fourier Transform of a function displays the amplitude (amount) of each frequency present in the underlying function (signal).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_23.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_23.markups.0":{"type":"STRONG","start":4,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_24":{"id":"3540af3bc32f_24","name":"4982","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*e3Wz9oS14NtCHTjF","typename":"ImageMetadata"},"text":"Top: a digital signal; Bottom: the Fourier Transform of the signal","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*e3Wz9oS14NtCHTjF":{"id":"0*e3Wz9oS14NtCHTjF","originalHeight":288,"originalWidth":432,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_25":{"id":"3540af3bc32f_25","name":"0e88","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"There are variants of the Fourier Transform including the Short-time fourier transform, which is implemented in the Librosa library and involves splitting an audio signal into frames and then taking the Fourier Transform of each frame. In audio processing generally, the Fourier is an elegant and useful way to decompose an audio signal into its constituent frequencies.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_25.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_25.markups.0":{"type":"A","start":58,"end":86,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FShort-time_Fourier_transform","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_26":{"id":"3540af3bc32f_26","name":"874e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"*Resources: by far the best video I’ve found on the Fourier Transform is from 3Blue1Brown*","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_26.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_26.markups.0":{"type":"A","start":78,"end":89,"href":"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=spUNpyF58BY&t=1s","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_27":{"id":"3540af3bc32f_27","name":"6802","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Periodogram","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_27.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_27.markups.0":{"type":"A","start":0,"end":11,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPeriodogram","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_28":{"id":"3540af3bc32f_28","name":"532c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*GJyquwcAOaD7pGRu","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*GJyquwcAOaD7pGRu":{"id":"0*GJyquwcAOaD7pGRu","originalHeight":267,"originalWidth":400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_29":{"id":"3540af3bc32f_29","name":"ef32","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In signal processing, a periodogram is an estimate of the spectral density of a signal. The periodogram above shows the power spectrum of two sinusoidal basis functions of ~30Hz and ~50Hz. The output of a Fourier Transform can be thought of as being (not exactly) essentially a periodogram.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_29.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_29.markups.0":{"type":"STRONG","start":24,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_30":{"id":"3540af3bc32f_30","name":"fdbd","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Spectral Density","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_30.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_30.markups.0":{"type":"A","start":0,"end":16,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSpectral_density","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_31":{"id":"3540af3bc32f_31","name":"e7af","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*jH4hECDI91m9HjHM","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*jH4hECDI91m9HjHM":{"id":"0*jH4hECDI91m9HjHM","originalHeight":843,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_32":{"id":"3540af3bc32f_32","name":"c1c7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The power spectrum of a time series is a way to describe the distribution of power into discrete frequency components composing that signal. The statistical average of a signal, measured by its frequency content, is called its spectrum. The spectral density of a digital signal describes the frequency content of the signal.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_32.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_32.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_32.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_32.markups.0":{"type":"STRONG","start":4,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_32.markups.1":{"type":"STRONG","start":227,"end":235,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_32.markups.2":{"type":"STRONG","start":241,"end":257,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_33":{"id":"3540af3bc32f_33","name":"90d3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Mel-Scale","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_33.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_33.markups.0":{"type":"A","start":0,"end":9,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMel_scale","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_34":{"id":"3540af3bc32f_34","name":"359e","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*abynK4WsXCMsQDby","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*abynK4WsXCMsQDby":{"id":"0*abynK4WsXCMsQDby","originalHeight":214,"originalWidth":450,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_35":{"id":"3540af3bc32f_35","name":"44db","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The mel-scale is a scale of pitches judged by listeners to be equal in distance from one another. The reference point between the mel-scale and normal frequency measurement is arbitrarily defined by assigning the perceptual pitch of 1000 mels to 1000 Hz. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments. The name mel comes from the word melody to indicate the scale is based on pitch comparisons.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_35.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_35.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_35.markups.0":{"type":"STRONG","start":4,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_35.markups.1":{"type":"STRONG","start":372,"end":375,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_36":{"id":"3540af3bc32f_36","name":"2307","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The formula to convert f hertz into m mels is:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_37":{"id":"3540af3bc32f_37","name":"26eb","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*DkrDP3PuoSfM07bf","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*DkrDP3PuoSfM07bf":{"id":"0*DkrDP3PuoSfM07bf","originalHeight":116,"originalWidth":430,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_38":{"id":"3540af3bc32f_38","name":"8720","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Cepstrum","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_38.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_38.markups.0":{"type":"A","start":0,"end":8,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCepstrum","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_39":{"id":"3540af3bc32f_39","name":"d86b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The cepstrum is the result of taking the Fourier Transform of the logarithm of the estimated power spectrum of a signal.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_39.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_39.markups.0":{"type":"STRONG","start":4,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_40":{"id":"3540af3bc32f_40","name":"b742","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Stectrogram","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_40.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_40.markups.0":{"type":"A","start":0,"end":11,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSpectrogram","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_41":{"id":"3540af3bc32f_41","name":"9111","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*KFcSrKDh4MCw5wG6","typename":"ImageMetadata"},"text":"Mel-frequency spectrogram of an audio sample in the Urbansound8k dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*KFcSrKDh4MCw5wG6":{"id":"0*KFcSrKDh4MCw5wG6","originalHeight":280,"originalWidth":665,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_42":{"id":"3540af3bc32f_42","name":"5e97","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. A nice way to think about spectrograms is as a stacked view of periodograms across some time-interval digital signal.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_42.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_42.markups.0":{"type":"STRONG","start":2,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_43":{"id":"3540af3bc32f_43","name":"f239","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Cochlea","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_43.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_43.markups.0":{"type":"A","start":0,"end":7,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCochlea","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_44":{"id":"3540af3bc32f_44","name":"dfd8","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The spiral cavity of the inner ear containing the organ of Corti, which produces nerve impulses in response to sound vibrations.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_45":{"id":"3540af3bc32f_45","name":"c16f","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Preprocessing Audio: Digital Signal Processing Techniques","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_45.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_45.markups.0":{"type":"STRONG","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_46":{"id":"3540af3bc32f_46","name":"19d9","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Dataset preprocessing, feature extraction and feature engineering are steps we take to extract information from the underlying data, information that in a machine learning context should be useful for predicting the class of a sample or the value of some target variable. In audio analysis this process is largely based on finding components of an audio signal that can help us distinguish it from other signals.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_47":{"id":"3540af3bc32f_47","name":"e70f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"MFCCs, as mentioned above, remain a state of the art tool for extracting information from audio samples. Despite libraries like Librosa giving us a python one-liner to compute MFCCs for an audio sample, the underlying math is a bit complicated, so we’ll go through it step by step and include some useful links for further learning.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_48":{"id":"3540af3bc32f_48","name":"ee4f","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Steps for calculating MFCCs for a given audio sample:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_49":{"id":"3540af3bc32f_49","name":"0e23","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Slice the signal into short frames (of time)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_50":{"id":"3540af3bc32f_50","name":"7279","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Compute the periodogram estimate of the power spectrum for each frame","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_51":{"id":"3540af3bc32f_51","name":"0e5f","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Apply the mel filterbank to the power spectra and sum the energy in each filter","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_52":{"id":"3540af3bc32f_52","name":"7e67","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Take the discrete cosine transform (DCT) of the log filterbank energies","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_53":{"id":"3540af3bc32f_53","name":"60f3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Excellent additional reading on MFCC derivation and computation can be found at blog posts here and here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_53.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_53.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_53.markups.0":{"type":"A","start":91,"end":95,"href":"http:\u002F\u002Fpracticalcryptography.com\u002Fmiscellaneous\u002Fmachine-learning\u002Fguide-mel-frequency-cepstral-coefficients-mfccs\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_53.markups.1":{"type":"A","start":100,"end":104,"href":"https:\u002F\u002Fhaythamfayek.com\u002F2016\u002F04\u002F21\u002Fspeech-processing-for-machine-learning.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_54":{"id":"3540af3bc32f_54","name":"2a4e","__typename":"Paragraph","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Slice the signal into short frames","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_54.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_54.markups.0":{"type":"STRONG","start":0,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_55":{"id":"3540af3bc32f_55","name":"dfa2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Slicing the audio signal into short frames is useful in that it allows us to sample our audio into discrete time-steps. We assume that on short enough time scales the audio signal doesn’t change. Typical values for the duration of the short frames are between 20–40ms. It is also conventional to overlap each frame 10–15ms.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_55.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_55.markups.0":{"type":"EM","start":77,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_56":{"id":"3540af3bc32f_56","name":"f252","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"*Note that the overlapping frames will make the features we eventually generate highly correlated. This is the basis for why we have to take the discrete cosine transform at the end of all of this.*","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_56.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_56.markups.0":{"type":"EM","start":0,"end":198,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_57":{"id":"3540af3bc32f_57","name":"65a6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"2. Compute the power spectrum for each frame","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_57.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_57.markups.0":{"type":"STRONG","start":0,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_58":{"id":"3540af3bc32f_58","name":"4de6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we have our frames we need to calculate the power spectrum of each frame. The power spectrum of a time series describes the distribution of power into frequency components composing that signal. According to Fourier analysis, any physical signal can be decomposed into a number of discrete frequencies, or a spectrum of frequencies over a continuous range. The statistical average of a certain signal as analyzed in terms of its frequency content is called its spectrum.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_59":{"id":"3540af3bc32f_59","name":"8ddd","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*McV5MrqouAwVb6nn","typename":"ImageMetadata"},"text":"Source: University of Maryland, Harmonic Analysis and the Fourier Transform","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_59.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*McV5MrqouAwVb6nn":{"id":"0*McV5MrqouAwVb6nn","originalHeight":420,"originalWidth":560,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_59.markups.0":{"type":"A","start":8,"end":75,"href":"https:\u002F\u002Fterpconnect.umd.edu\u002F~toh\u002Fspectrum\u002FHarmonicAnalysis.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_60":{"id":"3540af3bc32f_60","name":"9fc8","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We apply the Short-time fourier transform to each frame to obtain a power spectra for each.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_60.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_60.markups.0":{"type":"STRONG","start":13,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_61":{"id":"3540af3bc32f_61","name":"cdbf","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"3. Apply the mel filterbank to the power spectra and sum the energy in each filter","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_61.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_61.markups.0":{"type":"STRONG","start":0,"end":82,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_62":{"id":"3540af3bc32f_62","name":"b15f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We still have some work to do once we have our power spectra. The human cochlea does not discern between nearby frequencies well, and this effect only becomes more pronounced as frequencies increase. The mel-scale is a tool that allows us to approximate the human auditory system’s response more closely than linear frequency bands.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_62.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_62.markups.0":{"type":"STRONG","start":204,"end":213,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_63":{"id":"3540af3bc32f_63","name":"4bfe","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*ENJC6MU0Tot6ctyh","typename":"ImageMetadata"},"text":"Source: Columbia","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_63.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*ENJC6MU0Tot6ctyh":{"id":"0*ENJC6MU0Tot6ctyh","originalHeight":215,"originalWidth":415,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_63.markups.0":{"type":"A","start":8,"end":16,"href":"https:\u002F\u002Flabrosa.ee.columbia.edu\u002Fdoc\u002FHTKBook21\u002Fnode54.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_64":{"id":"3540af3bc32f_64","name":"3dd2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"As can be seen in the visualization above, the mel filters get wider as the frequency increases — we care less about variations at higher frequencies. At low frequencies, where differences are more discernible to the human ear and thus more important in our analysis, the filters are narrow.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_65":{"id":"3540af3bc32f_65","name":"4117","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The magnitudes from our power spectra, which were found by applying the Fourier transform to our input data, are binned by correlating them with each triangular Mel filter. This binning is usually applied such that each coefficient is multiplied by the corresponding filter gain, so each Mel filter comes to hold a weighted sum representing the spectral magnitude in that channel.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_65.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_65.markups.0":{"type":"A","start":113,"end":119,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FData_binning","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_66":{"id":"3540af3bc32f_66","name":"144d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we have our filterbank energies, we take the logarithm of each. This is yet another step motivated by the constraints of human hearing: humans don’t perceive changes in volume on a linear scale. To double the perceived volume of an audio wave, the wave’s energy must increase by a factor of 8. If an audiowave is already high volume (high energy), large variations in that wave’s energy may not sound very different.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_67":{"id":"3540af3bc32f_67","name":"7d5d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"4. Take the discrete cosine transform (DCT) of the log filterbank energies","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_67.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_67.markups.0":{"type":"STRONG","start":0,"end":74,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_68":{"id":"3540af3bc32f_68","name":"e195","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Because our filterbank energies are overlapping (see step 1), there is usually a strong correlation between them. Taking the discrete cosine transform can help decorrelate the energies.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_69":{"id":"3540af3bc32f_69","name":"ab3f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"*****","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_70":{"id":"3540af3bc32f_70","name":"4392","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Thankfully for us, the creators of Librosa have abstracted out a ton of this math and made it easy to generate MFCCs for your audio data. Let’s go through a simple python example to show how this analysis looks in action.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_70.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_70.markups.0":{"type":"A","start":35,"end":42,"href":"https:\u002F\u002Flibrosa.github.io\u002Flibrosa\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_71":{"id":"3540af3bc32f_71","name":"f1af","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"EXAMPLE PROJECT: Urbansound8k + Librosa","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_71.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_71.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_72":{"id":"3540af3bc32f_72","name":"b824","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We’re going to be fitting a simple neural network (keras + tensorflow backend) to the UrbanSound8k dataset. To begin let’s load our dependencies, including numpy, pandas, keras, scikit-learn, and librosa.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_73":{"id":"3540af3bc32f_73","name":"10de","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"#### Dependencies ####\n\n#### Import Comet for experiment tracking and visual tools\nfrom comet_ml import Experiment\n####\n\nimport IPython.display as ipd\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile as wav\n\nfrom sklearn import metrics \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_74":{"id":"3540af3bc32f_74","name":"d9f4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"To begin, let’s create a Comet experiment as a wrapper for all of our work. We’ll be able to capture any and all artifacts (audio files, visualizations, model, dataset, system information, training metrics, etc.) automatically.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_75":{"id":"3540af3bc32f_75","name":"2c02","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"experiment = Experiment(api_key=\"API_KEY\",\n                        project_name=\"urbansound8k\")","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_76":{"id":"3540af3bc32f_76","name":"f6fe","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s load in the dataset and grab a sample for each class from the dataset. We can inspect these samples visually and acoustically using Comet.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_77":{"id":"3540af3bc32f_77","name":"60fb","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Load dataset\ndf = pd.read_csv('UrbanSound8K\u002Fmetadata\u002FUrbanSound8K.csv')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_78":{"id":"3540af3bc32f_78","name":"f178","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Create a list of the class labels\nlabels = list(df['class'].unique())\n\n# Let's grab a single audio file from each class\nfiles = dict()\nfor i in range(len(labels)):\n    tmp = df[df['class'] == labels[i]][:1].reset_index()\n    path = 'UrbanSound8K\u002Faudio\u002Ffold{}\u002F{}'.format(tmp['fold'][0], tmp['slice_file_name'][0])\n    files[labels[i]] = path","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_79":{"id":"3540af3bc32f_79","name":"95f0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We can look at the waveforms for each sample using librosa’s display.waveplot function.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_80":{"id":"3540af3bc32f_80","name":"2700","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"fig = plt.figure(figsize=(15,15))# Log graphic of waveforms to Comet\nexperiment.log_image('class_examples.png')\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i, label in enumerate(labels):\n    fn = files[label]\n    fig.add_subplot(5, 2, i+1)\n    plt.title(label)\n    data, sample_rate = librosa.load(fn)\n    librosa.display.waveplot(data, sr= sample_rate)\nplt.savefig('class_examples.png')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_81":{"id":"3540af3bc32f_81","name":"f7d0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll save this graphic to our Comet experiment.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_82":{"id":"3540af3bc32f_82","name":"ef61","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Log graphic of waveforms to Comet\nexperiment.log_image('class_examples.png')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_83":{"id":"3540af3bc32f_83","name":"2dce","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*gRmnay8M3EBAwX_j","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*gRmnay8M3EBAwX_j":{"id":"0*gRmnay8M3EBAwX_j","originalHeight":1080,"originalWidth":1080,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_84":{"id":"3540af3bc32f_84","name":"be1f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Next, we’ll log the audio files themselves.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_85":{"id":"3540af3bc32f_85","name":"e645","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Log audio files to Comet for debugging\nfor label in labels:\n    fn = files[label]\n    experiment.log_audio(fn, metadata = {'name': label})","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_86":{"id":"3540af3bc32f_86","name":"04db","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we log the samples to Comet, we can listen to samples, inspect metadata, and much more right from the UI.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_87":{"id":"3540af3bc32f_87","name":"454c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*0y39WDJ_ChDIFKu-","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*0y39WDJ_ChDIFKu-":{"id":"0*0y39WDJ_ChDIFKu-","originalHeight":748,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_88":{"id":"3540af3bc32f_88","name":"fe2f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Preprocessing","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_88.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_88.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_89":{"id":"3540af3bc32f_89","name":"d9a0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we can extract features from our data. We’re going to be using librosa, but we’ll also show another utility, scipy.io, for comparison and to observe some implicit preprocessing that’s happening.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_90":{"id":"3540af3bc32f_90","name":"00ee","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"fn = 'UrbanSound8K\u002Faudio\u002Ffold1\u002F191431-9-0-66.wav'\nlibrosa_audio, librosa_sample_rate = librosa.load(fn)\nscipy_sample_rate, scipy_audio = wav.read(fn)\n\nprint(\"Original sample rate: {}\".format(scipy_sample_rate))\nprint(\"Librosa sample rate: {}\".format(librosa_sample_rate))","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_91":{"id":"3540af3bc32f_91","name":"bb91","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Original sample rate: 48000\nLibrosa sample rate: 22050","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_92":{"id":"3540af3bc32f_92","name":"3b5f","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Librosa’s load function will convert the sampling rate to 22.05 KHz automatically. It will also normalize the bit depth between -1 and 1.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_93":{"id":"3540af3bc32f_93","name":"f6e1","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"print('Original audio file min~max range: {} to {}'.format(np.min(scipy_audio), np.max(scipy_audio)))","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_94":{"id":"3540af3bc32f_94","name":"a00e","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"print('Librosa audio file min~max range: {0:.2f} to {0:.2f}'.format(np.min(librosa_audio), np.max(librosa_audio)))","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_95":{"id":"3540af3bc32f_95","name":"cb82","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"\u003EOriginal audio file min~max range: -1869 to 1665\n\u003E Librosa audio file min~max range: -0.05 to -0.05","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_96":{"id":"3540af3bc32f_96","name":"9ba2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Librosa also converts the audio signal to mono from stereo.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_97":{"id":"3540af3bc32f_97","name":"100d","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"plt.figure(figsize=(12, 4))\nplt.plot(scipy_audio)\nplt.savefig('original_audio.png')\nexperiment.log_image('original_audio.png')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_98":{"id":"3540af3bc32f_98","name":"0677","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*khv78FDS0CTmlwgO","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*khv78FDS0CTmlwgO":{"id":"0*khv78FDS0CTmlwgO","originalHeight":252,"originalWidth":738,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_99":{"id":"3540af3bc32f_99","name":"62fc","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Original Audio (note that it’s in stereo — two audio sources)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_99.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_99.markups.0":{"type":"EM","start":0,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_100":{"id":"3540af3bc32f_100","name":"b3b2","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Librosa: mono track\nplt.figure(figsize=(12,4))\nplt.plot(librosa_audio)\nplt.savefig('librosa_audio.png')\nexperiment.log_image('librosa_audio.png')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_101":{"id":"3540af3bc32f_101","name":"50a0","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*NTu8BYsPZmHWIwL7","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*NTu8BYsPZmHWIwL7":{"id":"0*NTu8BYsPZmHWIwL7","originalHeight":252,"originalWidth":725,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_102":{"id":"3540af3bc32f_102","name":"287b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Librosa audio: converted to mono","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_102.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_102.markups.0":{"type":"EM","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_103":{"id":"3540af3bc32f_103","name":"479b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Extracting MFCCs from audio using Librosa","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_103.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_103.markups.0":{"type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_104":{"id":"3540af3bc32f_104","name":"cb14","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Remember all the math we went through to understand mel-frequency cepstrum coefficients earlier? Using Librosa, here’s how you extract them from audio (using the librosa_audio we defined above)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_105":{"id":"3540af3bc32f_105","name":"f14d","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc = 40)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_106":{"id":"3540af3bc32f_106","name":"d2a3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"That’s it!","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_107":{"id":"3540af3bc32f_107","name":"7fcf","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"print(mfccs.shape)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_108":{"id":"3540af3bc32f_108","name":"f0b6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"\u003E (40, 173)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_109":{"id":"3540af3bc32f_109","name":"fce7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Librosa calculated 40 MFCCs over a 173 frame audio sample.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_110":{"id":"3540af3bc32f_110","name":"ad4d","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"plt.figure(figsize=(8,8))\nlibrosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')\nplt.savefig('MFCCs.png')\nexperiment.log_image('MFCCs.png')","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_111":{"id":"3540af3bc32f_111","name":"cf4d","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*GvSfqUQV0m6gFote","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*GvSfqUQV0m6gFote":{"id":"0*GvSfqUQV0m6gFote","originalHeight":480,"originalWidth":465,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_112":{"id":"3540af3bc32f_112","name":"90d6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll define a simple function to extract MFCCs for every file in our dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_113":{"id":"3540af3bc32f_113","name":"dadd","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def extract_features(file_name):","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_114":{"id":"3540af3bc32f_114","name":"25cc","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n     \n    return mfccs_processed","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_115":{"id":"3540af3bc32f_115","name":"6ff6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Now let’s extract features.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_116":{"id":"3540af3bc32f_116","name":"4a3f","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"features = []\n\n# Iterate through each sound file and extract the features \nfor index, row in metadata.iterrows():","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_117":{"id":"3540af3bc32f_117","name":"86fa","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'\u002F',str(row[\"slice_file_name\"]))\n    \n    class_label = row[\"class\"]\n    data = extract_features(file_name)\n    \n    features.append([data, class_label])\n\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_118":{"id":"3540af3bc32f_118","name":"e261","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We now have a dataframe where each row has a label (class) and a single feature column, comprised of 40 MFCCs.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_119":{"id":"3540af3bc32f_119","name":"c495","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"featuresdf.head()","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_120":{"id":"3540af3bc32f_120","name":"5629","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*2tt2pt60eGJ_LpzT","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*2tt2pt60eGJ_LpzT":{"id":"0*2tt2pt60eGJ_LpzT","originalHeight":350,"originalWidth":952,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_121":{"id":"3540af3bc32f_121","name":"13a0","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"featuresdf.iloc[0]['feature']","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_122":{"id":"3540af3bc32f_122","name":"145e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"array([-2.1579300e+02, 7.1666122e+01, -1.3181377e+02, -5.2091331e+01,-2.2115969e+01, -2.1764181e+01, -1.1183747e+01, 1.8912683e+01,6.7266388e+00, 1.4556893e+01, -1.1782045e+01, 2.3010368e+00, -1.7251305e+01, 1.0052421e+01, -6.0095000e+00, -1.3153191e+00, -1.7693510e+01, 1.1171228e+00, -4.3699470e+00, 7.2629538e+00, -1.1815971e+01, -7.4952612e+00, 5.4577131e+00, -2.9442446e+00, -5.8693886e+00, -9.8654032e-02, -3.2121708e+00, 4.6092505e+00, -5.8293257e+00, -5.3475075e+00, 1.3341187e+00, 7.1307826e+00, -7.9450034e-02, 1.7109241e+00, -5.6942000e+00, -2.9041715e+00, 3.0366952e+00, -1.6827590e+00, -8.8585770e-01, 3.5438776e-01], dtype=float32)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_123":{"id":"3540af3bc32f_123","name":"bc01","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have successfully extracted our features from the underlying audio data, we can build and train a model.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_124":{"id":"3540af3bc32f_124","name":"9e24","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Model building and training","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_124.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_124.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_125":{"id":"3540af3bc32f_125","name":"c9ed","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We’ll start by converting our MFCCs to numpy arrays, and encoding our classification labels.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_126":{"id":"3540af3bc32f_126","name":"6da7","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# Convert features and corresponding classification labels into numpy arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y))","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_127":{"id":"3540af3bc32f_127","name":"00bc","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Our dataset will be split into training and test sets.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_128":{"id":"3540af3bc32f_128","name":"4a1e","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 127)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_129":{"id":"3540af3bc32f_129","name":"1460","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s define and compile a simple feedforward neural network architecture.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_130":{"id":"3540af3bc32f_130","name":"7f48","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"num_labels = yy.shape[1]\nfilter_size = 2","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_131":{"id":"3540af3bc32f_131","name":"71f8","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def build_model_graph(input_shape=(40,)):\n    model = Sequential()\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_labels))\n    model.add(Activation('softmax'))\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n    return model","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_132":{"id":"3540af3bc32f_132","name":"f8ff","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model = build_model_graph()","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_133":{"id":"3540af3bc32f_133","name":"fce2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s look at a model summary and compute pre-training accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_134":{"id":"3540af3bc32f_134","name":"5224","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Display model architecture summary \nmodel.summary()\n\n# Calculate pre-training accuracy \nscore = model.evaluate(x_test, y_test, verbose=0)\naccuracy = 100*score[1]","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_135":{"id":"3540af3bc32f_135","name":"77ed","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*uRGk70cCRYTd7mJs","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*uRGk70cCRYTd7mJs":{"id":"0*uRGk70cCRYTd7mJs","originalHeight":862,"originalWidth":1160,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_136":{"id":"3540af3bc32f_136","name":"fbb5","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"print(\"Pre-training accuracy: %.4f%%\" % accuracy)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_137":{"id":"3540af3bc32f_137","name":"e4e3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Pre-training accuracy: 12.2496%","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_138":{"id":"3540af3bc32f_138","name":"ae02","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Now it’s time to train our model.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_139":{"id":"3540af3bc32f_139","name":"c020","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from keras.callbacks import ModelCheckpoint \nfrom datetime import datetime \n\nnum_epochs = 100\nnum_batch_size = 32\n\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_140":{"id":"3540af3bc32f_140","name":"7847","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Training completed in time:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_141":{"id":"3540af3bc32f_141","name":"1c96","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Even before training completed, Comet keeps track of the key information about our experiment. We can visualize our accuracy and loss curves in real time from the Comet UI (note the orange spin wheel indicates that training is in process).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_142":{"id":"3540af3bc32f_142","name":"51ac","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:0*eE8BpCleWP1dXCOL","typename":"ImageMetadata"},"text":"Comet’s experiment visualization dashboard","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*eE8BpCleWP1dXCOL":{"id":"0*eE8BpCleWP1dXCOL","originalHeight":706,"originalWidth":1400,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:3540af3bc32f_143":{"id":"3540af3bc32f_143","name":"7cbc","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Once trained we can evaluate our model on the train and test data.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_144":{"id":"3540af3bc32f_144","name":"d0e2","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: {0:.2%}\".format(score[1]))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: {0:.2%}\".format(score[1]))","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_145":{"id":"3540af3bc32f_145","name":"c8d2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Training Accuracy: 93.00%\nTesting Accuracy: 87.35%","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_146":{"id":"3540af3bc32f_146","name":"8618","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_146.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_146.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_147":{"id":"3540af3bc32f_147","name":"ea9b","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Our model has trained rather well, but there is likely lots of room for improvement, perhaps using Comet’s Hyperparameter Optimization tool. In a small amount of code we’ve been able to extract mathematically complex MFCCs from audio data, build and train a neural network to classify audio based on those MFCCs, and evaluate our model on the test data.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_147.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_147.markups.0":{"type":"A","start":107,"end":134,"href":"https:\u002F\u002Fwww.comet.ml\u002Fdocs\u002Fpython-sdk\u002Fintroduction-optimizer\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_148":{"id":"3540af3bc32f_148","name":"961e","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"To get started with Comet, click here. Comet is 100% FREE for public projects.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_148.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_148.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_148.markups.0":{"type":"A","start":27,"end":38,"href":"https:\u002F\u002Fwww.comet.ml\u002Fpricing?opensignup=true","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_148.markups.1":{"type":"STRONG","start":0,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:3540af3bc32f_149":{"id":"3540af3bc32f_149","name":"1099","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"If you are interested in our Enterprise product, which can be installed on-premise, schedule a demo here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:3540af3bc32f_149.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"Paragraph:3540af3bc32f_149.markups.0":{"type":"A","start":84,"end":104,"href":"https:\u002F\u002Fcalendly.com\u002Fniko_comet","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:data-science":{"id":"data-science","displayTitle":"Data Science","__typename":"Tag"},"Tag:programming":{"id":"programming","displayTitle":"Programming","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"ImageMetadata:":{"id":"","__typename":"ImageMetadata"},"$Post:e160b069e88.previewContent":{"subtitle":"Author: Niko Laskaris, Customer Facing Data Scientist, Comet.ml","__typename":"PreviewContent"}}
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/manifest.2e49f3a9.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/vendors~main.8289b224.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/main.6045f352.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/vendors~screen.collection.packageBuilder~screen.landingpages.pres45~screen.landingpages.tribute~scre~3e410f11.6e718f1d.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/screen.collection.packageBuilder~screen.landingpages.pres45~screen.landingpages.tribute~screen.post~~73c4bb05.09146537.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/screen.post~screen.post.amp~screen.post.series~screen.profile~screen.sequence.library~screen.sequenc~036c6b37.5b1f9621.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/screen.collection.packageBuilder~screen.landingpages.pres45~screen.landingpages.tick~screen.landingp~ed90ee39.40edb6c7.chunk.js">
  </script>
  <script src="https://cdn-client.medium.com/lite/static/js/screen.post.2745e55e.chunk.js">
  </script>
  <script>
   window.main();
  </script>
 </body>
</html>